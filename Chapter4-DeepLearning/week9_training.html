
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Deep Neural Networks and their training &#8212; ML Geo Curriculum</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=62ba249389abaaa9ffc34bf36a076bdc1d65ee18" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=f31d14ad54b65d19161ba51d4ffff3a77ae00456"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Using PyTorch to build, train, and use neural networks: A short introduction" href="neural_networks_PyTorch.html" />
    <link rel="prev" title="Training Models" href="ModelTraining.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/GeoSMART_logo.svg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">ML Geo Curriculum</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../about_this_book/about_this_book.html">
                    Machine Learning in the Geosciences
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  About this Book
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference external" href="https://geo-smart.github.io/index.html">
   Geosmart website
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../about_this_book/acknowledgements.html">
   Acknowlegments
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Chapter 1 - Open Source Ecosystem with Python
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter1-GettingStarted/readme.html">
   Getting Started
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter1-GettingStarted/1.1_python_environment.html">
   1.1 Python Ecosystem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter1-GettingStarted/1.2_jupyter_environment.html">
   1.2 Jupyter Environment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter1-GettingStarted/1.3_version_control_git.html">
   1.3 Version Control &amp; GitHub
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter1-GettingStarted/1.4_computational_environments.html">
   1.4 Computing Environments
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Chapter 2 - Data Manipulation
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter2-DataManipulation/2.1_Data_Definitions.html">
   2.1 Data Definitions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter2-DataManipulation/2.2_Numpy_arrays.html">
   2.2 Numpy Arrays
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter2-DataManipulation/2.3_pandas_rendered.html">
   2.3 Pandas, Basic Mapping
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter2-DataManipulation/2.4_xarrays.html">
   2.4 Xarrays
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter2-DataManipulation/2.5_data_formats.html">
   2.5 Data Formats
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter2-DataManipulation/2.6_resampling.html">
   2.6 Resampling Methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter2-DataManipulation/2.8_feature_engineering.html">
   2.6: Feature engineering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter2-DataManipulation/2.9_pca.html">
   2.7 Principal Component Analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter2-DataManipulation/2.10_dimension_reduction.html">
   2.9 Dimensionality Reduction
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Chapter 3 - Machine Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter3-MachineLearning/logistic_regression.html">
   Logistic regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter3-MachineLearning/kmeans.html">
   K-means clustering - Tutorial
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Chapter 4 - Deep Learning
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="MultiLayerPerceptron.html">
   Multi Layer Perceptrons
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="week9_cnn.html">
   Convolutional Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="cnn_LeNet.html">
   Convolutional neural networks with PyTorch
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="autoen.html">
   Auto-encoders
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ModelTraining.html">
   Training Models
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Deep Neural Networks and their training
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="neural_networks_PyTorch.html">
   Using PyTorch to build, train, and use neural networks: A short introduction
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Chapter 5 - Model Evaluation
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter5-ModelEvaluation/week7_classification_model_fit.html">
   Classification
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Chapter 6 - Workflow Management and Reproducibility
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter6-ModelWorkflows/readme.html">
   This chapter focuces on model workflow and ML reproducibility
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Chapter 7- Introduction to Cloud Computing
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference external" href="https://github.com/cloudmaven">
   Browser Access to Cloud Instances
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference external" href="https://github.com/Denolle-Lab/azure">
   Terraform Access to Cloud Instances
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference external" href="https://tljh.jupyter.org/en/latest/">
   Cloud Provider ML Jupyterhubs
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Reference
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../reference/glossary.html">
   Glossaries
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../reference/bibliography.html">
   Bibliography
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/geo-smart/curriculum-book/main?urlpath=lab/tree/book/Chapter4-DeepLearning/week9_training.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/geo-smart/curriculum-book"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/geo-smart/curriculum-book/issues/new?title=Issue%20on%20page%20%2FChapter4-DeepLearning/week9_training.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/geo-smart/curriculum-book/edit/main/book/Chapter4-DeepLearning/week9_training.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Edit this page"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="headerbtn__text-container">suggest edit</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/Chapter4-DeepLearning/week9_training.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Deep Neural Networks and their training
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#layer-initialization">
     1. Layer initialization
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#activation-functions">
     2. Activation functions
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#batch-normalization">
   3. Batch Normalization
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#clipping-the-gradients">
   4. Clipping the gradients
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#transfer-learning">
   5. Transfer learning
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optimizers">
   Optimizers
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#avoiding-overfitting-for-training-dnns">
   Avoiding Overfitting for training DNNs
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#regularizations">
     Regularizations
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dropout">
   Dropout
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#marine-will-update-this-section">
     marine will update this section
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Deep Neural Networks and their training</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Deep Neural Networks and their training
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#layer-initialization">
     1. Layer initialization
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#activation-functions">
     2. Activation functions
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#batch-normalization">
   3. Batch Normalization
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#clipping-the-gradients">
   4. Clipping the gradients
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#transfer-learning">
   5. Transfer learning
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optimizers">
   Optimizers
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#avoiding-overfitting-for-training-dnns">
   Avoiding Overfitting for training DNNs
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#regularizations">
     Regularizations
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dropout">
   Dropout
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#marine-will-update-this-section">
     marine will update this section
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="deep-neural-networks-and-their-training">
<h1>Deep Neural Networks and their training<a class="headerlink" href="#deep-neural-networks-and-their-training" title="Permalink to this headline">#</a></h1>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>⚠️ Under Construction !</p>
</div>
<p>A neural network is considered “deep” if it has “many” layers (more than 5).</p>
<p>Shiny new methods like DNN are reported with amazing performance and accuracy. Be aware that only the best performance get put in papers, the majority of the time is spent with failures on model design and training.</p>
<p>A major issue with DNN stems from its dependence on the gradient of the cost functions. Because the gradients uses the chain rule with multiplication of errors in each layer, they can vanish or explode if the variance increases with each layer. It happens that the logistic function (sigmoid) having a mean of 0.5, and saturating quickly to either 0 or 1 (gradients of the sigmoid tends to zero quickly).</p>
<p>The training of deep neural networks is effectively a conditioning of the model weights to satisfy model accuracy, generalization, and computational efficiency. To strike that balance requires extensive tuning of hyperparameters, a goal to keep models as simple as possible, while aiming the best generalization. The conditioning of model parameters means that their values are not too small (unless intentionally sparse), mostly not too large, and that the variance of these values remains low.</p>
<p>This notebook covers important best practice guidelines for training deep neural networks.</p>
<ul class="simple">
<li><p>Model initialization</p></li>
<li><p>Activation functions</p></li>
<li><p>Batch normalization</p></li>
<li><p>Optimizers</p></li>
<li><p>Transfer learning</p></li>
<li><p>Dropout (for training and for epistemic uncertainty calculations)</p></li>
</ul>
<section id="layer-initialization">
<h2>1. Layer initialization<a class="headerlink" href="#layer-initialization" title="Permalink to this headline">#</a></h2>
<p>Glorot and He (2010) proposes an initialization that helps lower the variance of the outputs in each layer and of the model. The number of inputs (fan<sub>in</sub> ) and the numbers of neurons (fan<sub>out</sub>). fan<sub>avg</sub> = (fan<sub>in</sub> +fan<sub>out</sub> )/2. By default, Keras uses a Glorot initialization with a uniform distribution. The following table lists the good pairing between initialization and activation functions.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>initialization</p></th>
<th class="head"><p>activation functions</p></th>
<th class="head"><p>distribution width (<span class="math notranslate nohighlight">\(\sigma^2\)</span>)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Glorot</p></td>
<td><p>None,tanh,logistic,softmax</p></td>
<td><p>1/fan<sub>avg</sub></p></td>
</tr>
<tr class="row-odd"><td><p>He</p></td>
<td><p>Relu &amp; variants</p></td>
<td><p>1/fan<sub>in</sub></p></td>
</tr>
<tr class="row-even"><td><p>LeCun</p></td>
<td><p>SELU</p></td>
<td><p>1/fan<sub>in</sub></p></td>
</tr>
</tbody>
</table>
<p>One can initialize the layers using these distributions as kernels:</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div></blockquote>
<p>keras.layers.Dense(300,activation=”relu”,kernel_initializer=”he_normal”)</p>
<p>or choose a custom layer with a He inialization of uniform distribution based out of fan<sub>ave</sub> instead of fan<sub>in</sub>:</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div></blockquote>
<p>init = keras.initializers.VarianceScaling(scale=2.,model=’fan_avg’,distribution=’uniform’)</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div></blockquote>
<p>keras.layers.Dense(300,activation=”relu”,kernel_initializer=my_custom_init)</p>
<p>More information here: <a class="reference external" href="https://keras.io/api/layers/initializers/">https://keras.io/api/layers/initializers/</a></p>
</section>
<section id="activation-functions">
<h2>2. Activation functions<a class="headerlink" href="#activation-functions" title="Permalink to this headline">#</a></h2>
<p><strong>sigmoid/logistic</strong> : most popular at first for smooth activation, but their choice of saturation is not appropriate for DNN where the chain rule applied to the gradients of hte sigmoid tend to vanish.</p>
<p><strong>ReLu</strong> : Practical, but without other regularization mechanisms (batch normalization and dropout), they tend to kill certain neurons in the network.</p>
<p><strong>LeakyReLU</strong> : A variant of the ReLu that has a weak positive slope (hyper parameter <code class="docutils literal notranslate"><span class="pre">alpha</span></code>) for negative values. It avoids having a strict zero for negative values and avoids the satuation and vanishing of the gradients. Outperforms ReLU. Hard to tune the hyperparameter <span class="math notranslate nohighlight">\(\alpha\)</span>. To use Leaky ReLU, you need to add a layer:</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div></blockquote>
<p>keras.layers.Dense(10,kernel_initializer=”he_normal”)
keras.layers.LeakyReLU(alpha=0.2)```</p>
<p><strong>Randomized Leaky ReLU - RReLU</strong> : The hyperparameter <span class="math notranslate nohighlight">\(\alpha\)</span> is randomized during training, fixed during testing. RRLeu acts as a regularizer and reduces the risk of overfitting.</p>
<p><strong>Parametric leaky ReLU (PReLu)</strong>: The hyper parameter <code class="docutils literal notranslate"><span class="pre">alpha</span></code> is learned during training. Outperforms ReLU for large data sets, runs the risk to overfit the smaller data sets.</p>
<p><strong>Exponential Linear Unit ELU</strong>: Instead of a linear trend in the negative values (as in the LeakyReLU), it is an exponential that smoothly saturates to a negative value. It outperforms all of the variants of ReLUs: training time reduced, never kills the neurons, smooth everywhere if <code class="docutils literal notranslate"><span class="pre">alpha=1</span></code> that helps speed up GD. A major drawback is the computational expense, but it is compensated by faster convergence.</p>
<p><strong>Scaled ELU - SELU</strong> (Klambauer et al, 2017): a scaled variant of ELU. It self-normalizes the network because the output of each layer will tend to preserve a zero mean. However, it requires: input features must be standardized (mean 0, std=1), initialization of weights must be LeCun normal (<code class="docutils literal notranslate"><span class="pre">kernel_initializer=&quot;lecun_normal&quot;</span></code>), network must be sequential (no skip connection, nor RNN, no wide nets), layers must be dense.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="o">%</span><span class="k">matplotlib</span> inline

<span class="n">x</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1000</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">sigm</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> <span class="c1"># define the sigmoid function</span>
    <span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
<span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> <span class="c1"># define the Rectified Linear Unit function</span>
    <span class="k">return</span>   <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">Leakyrelu</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">x</span><span class="p">):</span> <span class="c1"># define the Leaky Rectified Linear Unit function</span>
    <span class="k">return</span>  <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">a</span><span class="o">*</span><span class="n">x</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">Leakyrelu</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">x</span><span class="p">):</span> <span class="c1"># define the Leaky Rectified Linear Unit function</span>
    <span class="k">return</span>  <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">a</span><span class="o">*</span><span class="n">x</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">elu</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">x</span><span class="p">):</span> <span class="c1"># define the Exponential Linear Unit function</span>
    <span class="k">return</span>  <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">a</span><span class="o">*</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
                       
<span class="n">s</span> <span class="o">=</span> <span class="n">sigm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">r</span> <span class="o">=</span> <span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">lr</span> <span class="o">=</span> <span class="n">Leakyrelu</span><span class="p">(</span><span class="mf">0.2</span><span class="p">,</span><span class="n">x</span><span class="p">)</span>
<span class="n">e</span> <span class="o">=</span> <span class="n">elu</span><span class="p">(</span><span class="mf">0.2</span><span class="p">,</span><span class="n">x</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">s</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">r</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">lr</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">t</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">e</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">,</span><span class="s1">&#39;ReLu&#39;</span><span class="p">,</span><span class="s1">&#39;Leaky ReLU&#39;</span><span class="p">,</span><span class="s1">&#39;TanH&#39;</span><span class="p">,</span><span class="s1">&#39;ELU&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;X&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;activation functions&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0.5, 1.0, &#39;activation functions&#39;)
</pre></div>
</div>
<img alt="../_images/week9_training_1_1.png" src="../_images/week9_training_1_1.png" />
</div>
</div>
<p>In general, the preferred choice should be:</p>
<p><strong>SELU (if dense and sequential) &gt; ELU &gt; Leaky ReLU (and variants) &gt; ReLU &gt; tanh &gt; logistic.</strong></p>
<p>ReLU is the most implemented, and most packages have optimizers that best perform on ReLU, so a safe choice is ReLU. More information here: <a class="reference external" href="https://keras.io/api/layers/activation_layers/">https://keras.io/api/layers/activation_layers/</a></p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="batch-normalization">
<h1>3. Batch Normalization<a class="headerlink" href="#batch-normalization" title="Permalink to this headline">#</a></h1>
<p>The BN layer standardizes the inputs (shift and normalize the outputs of the layers). It adds parameters and complexity to the network. The benefits are:</p>
<ul class="simple">
<li><p>it reduces vanishing/exploding gradients (one can now use saturating activation functions),</p></li>
<li><p>networks are less sensitive to the weight initialization,</p></li>
<li><p>one can use larger learning rates,</p></li>
<li><p>they act as a regularizer (reducing the need for regularization and dropout).</p></li>
</ul>
<p>Tips for its implementation:</p>
<ul class="simple">
<li><p>Whether the BN layer should be before or after the activation function seems to depend on the task, so one has to experiment with each data set.</p></li>
<li><p>To add the BN layer before the activation function, you must remove the activation function from the hidden layers and add them separately</p></li>
<li><p>Because BN removes the bias term from the previous layers, you can remove the bias term from the previous layers <code class="docutils literal notranslate"><span class="pre">use_bias=False</span></code>.</p></li>
<li><p>the hyperparameter <code class="docutils literal notranslate"><span class="pre">momentum</span></code>  that is used to update the averages &lt;1 but close to one 0.999s for large data sets and small mini batches.</p></li>
<li><p>BN layers by default acts on one axis, use the argument <code class="docutils literal notranslate"><span class="pre">axis=[1,2]</span></code> for 2D input batch (or flatten your 2D input batch to 1D).</p></li>
</ul>
<p>Here is an example:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tensorflow</span> <span class="kn">import</span> <span class="n">keras</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span> <span class="p">[</span>
<span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">[</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">]),</span> <span class="c1"># reshape the 2D matrix into a 1D vector, without modifying the values</span>
<span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">BatchNormalization</span><span class="p">(),</span>
<span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">300</span><span class="p">,</span><span class="n">kernel_initializer</span><span class="o">=</span><span class="s2">&quot;he_normal&quot;</span><span class="p">,</span><span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span> <span class="c1"># single dense layer, downsampling from input layer to this year from 784 points to 300.</span>
<span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">BatchNormalization</span><span class="p">(),</span>
<span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Activation</span><span class="p">(</span><span class="s2">&quot;relu&quot;</span><span class="p">),</span>
<span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span><span class="n">kernel_initializer</span><span class="o">=</span><span class="s2">&quot;he_normal&quot;</span><span class="p">,</span><span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span> <span class="c1"># 100 neurons</span>
<span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">BatchNormalization</span><span class="p">(),</span>
<span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Activation</span><span class="p">(</span><span class="s2">&quot;relu&quot;</span><span class="p">),</span>
<span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s2">&quot;softmax&quot;</span><span class="p">)</span> <span class="p">])</span> <span class="c1"># output layer, 10 neurons since there are 10 classes.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;sequential&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten (Flatten)            (None, 784)               0         
_________________________________________________________________
batch_normalization (BatchNo (None, 784)               3136      
_________________________________________________________________
dense (Dense)                (None, 300)               235200    
_________________________________________________________________
batch_normalization_1 (Batch (None, 300)               1200      
_________________________________________________________________
activation (Activation)      (None, 300)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 100)               30000     
_________________________________________________________________
batch_normalization_2 (Batch (None, 100)               400       
_________________________________________________________________
activation_1 (Activation)    (None, 100)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 10)                1010      
=================================================================
Total params: 270,946
Trainable params: 268,578
Non-trainable params: 2,368
_________________________________________________________________
None
</pre></div>
</div>
</div>
</div>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="clipping-the-gradients">
<h1>4. Clipping the gradients<a class="headerlink" href="#clipping-the-gradients" title="Permalink to this headline">#</a></h1>
<p>If Batch Normalization is tricky to use (e.g. in RNNs), one can simply clip the gradients so that they do not explode. You can clip the value of the gradients by a tunable/hyperparameter value <code class="docutils literal notranslate"><span class="pre">clipvalue=1.0</span></code> or by the L2 norm of the gradients <code class="docutils literal notranslate"><span class="pre">clipnorm=1.0</span></code> (it preserves the orientation of the gradient. Try both and see what works best.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.005</span><span class="c1"># learning rate lr</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span><span class="n">clipvalue</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span> <span class="c1"># stochastic gradient descent</span>
</pre></div>
</div>
</div>
</div>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="transfer-learning">
<h1>5. Transfer learning<a class="headerlink" href="#transfer-learning" title="Permalink to this headline">#</a></h1>
<p>Transfer learning is useful to improve model generalization, continue training networks, modify already trained networks and adapt.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_A</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="s2">&quot;my_first_NN_model.h5&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model_A</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;sequential_13&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_7 (Flatten)          (None, 784)               0         
_________________________________________________________________
dropout_3 (Dropout)          (None, 784)               0         
_________________________________________________________________
dense_28 (Dense)             (None, 300)               235500    
_________________________________________________________________
dropout_4 (Dropout)          (None, 300)               0         
_________________________________________________________________
dense_29 (Dense)             (None, 100)               30100     
_________________________________________________________________
dropout_5 (Dropout)          (None, 100)               0         
_________________________________________________________________
dense_30 (Dense)             (None, 10)                1010      
=================================================================
Total params: 266,610
Trainable params: 266,610
Non-trainable params: 0
_________________________________________________________________
None
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_B_on_A</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">model_A</span><span class="o">.</span><span class="n">layers</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="c1"># use all of the network exept the last year.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model_B_on_A</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
<span class="n">model_B_on_A</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s2">&quot;softmax&quot;</span><span class="p">))</span> <span class="c1"># this is your new model: here i reset the output layer</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model_B_on_A</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;sequential_1&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_7 (Flatten)          (None, 784)               0         
_________________________________________________________________
dropout_3 (Dropout)          (None, 784)               0         
_________________________________________________________________
dense_28 (Dense)             (None, 300)               235500    
_________________________________________________________________
dropout_4 (Dropout)          (None, 300)               0         
_________________________________________________________________
dense_29 (Dense)             (None, 100)               30100     
_________________________________________________________________
dropout_5 (Dropout)          (None, 100)               0         
=================================================================
Total params: 265,600
Trainable params: 265,600
Non-trainable params: 0
_________________________________________________________________
None
Model: &quot;sequential_1&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_7 (Flatten)          (None, 784)               0         
_________________________________________________________________
dropout_3 (Dropout)          (None, 784)               0         
_________________________________________________________________
dense_28 (Dense)             (None, 300)               235500    
_________________________________________________________________
dropout_4 (Dropout)          (None, 300)               0         
_________________________________________________________________
dense_29 (Dense)             (None, 100)               30100     
_________________________________________________________________
dropout_5 (Dropout)          (None, 100)               0         
_________________________________________________________________
dense_3 (Dense)              (None, 10)                1010      
=================================================================
Total params: 266,610
Trainable params: 266,610
Non-trainable params: 0
_________________________________________________________________
None
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_A_clone</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">clone_model</span><span class="p">(</span><span class="n">model_A</span><span class="p">)</span> <span class="c1"># CLONE MODELS TO AVOID OVERWRITING!</span>
<span class="n">model_A_clone</span><span class="o">.</span><span class="n">set_weights</span><span class="p">(</span><span class="n">model_A</span><span class="o">.</span><span class="n">get_weights</span><span class="p">())</span> <span class="c1"># COPY THE WEIGHT</span>
</pre></div>
</div>
</div>
</div>
<p>Training model_B_on_A will modify  model_A, thus one has to clone model_A to keep an unmodified version.</p>
<p>When you retrain but modified and added layers, there is an inbalance of weights or information: the old model is already optimized, the new layers are random. One can freeze the pre-trained layers to force the training on the newly added layers during a few epochs. One can freeze layers by switching the status of the parameters “trainable” from “True” to “False”</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">model_B_on_A</span><span class="o">.</span><span class="n">layers</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
    <span class="n">layer</span><span class="o">.</span><span class="n">trainable</span><span class="o">=</span><span class="kc">False</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model_B_on_A</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;sequential_3&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_7 (Flatten)          (None, 784)               0         
_________________________________________________________________
dropout_3 (Dropout)          (None, 784)               0         
_________________________________________________________________
dense_28 (Dense)             (None, 300)               235500    
_________________________________________________________________
dropout_4 (Dropout)          (None, 300)               0         
_________________________________________________________________
dense_29 (Dense)             (None, 100)               30100     
_________________________________________________________________
dropout_5 (Dropout)          (None, 100)               0         
_________________________________________________________________
dense_3 (Dense)              (None, 10)                1010      
=================================================================
Total params: 266,610
Trainable params: 1,010
Non-trainable params: 265,600
_________________________________________________________________
None
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.005</span><span class="c1"># learning rate lr</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span> <span class="c1"># stochastic gradient descent</span>
<span class="n">model_B_on_A</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s2">&quot;sparse_categorical_crossentropy&quot;</span><span class="p">,</span>
             <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
             <span class="n">metrics</span><span class="o">=</span><span class="s2">&quot;accuracy&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Train on a few epochs, then unfreeze the weights</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># load the fashion MNIST data. it&#39;s boring, but it works!</span>
<span class="n">fashion_mnist</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">fashion_mnist</span>
<span class="p">(</span><span class="n">X_train_full</span><span class="p">,</span> <span class="n">y_train_full</span><span class="p">),</span> <span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">fashion_mnist</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>
<span class="n">class_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;tshirt&quot;</span><span class="p">,</span><span class="s2">&quot;trousers&quot;</span><span class="p">,</span><span class="s2">&quot;pullover&quot;</span><span class="p">,</span><span class="s2">&quot;dress&quot;</span><span class="p">,</span><span class="s2">&quot;coat&quot;</span><span class="p">,</span><span class="s2">&quot;sandal&quot;</span><span class="p">,</span><span class="s2">&quot;shirt&quot;</span><span class="p">,</span><span class="s2">&quot;sneaker&quot;</span><span class="p">,</span><span class="s2">&quot;bag&quot;</span><span class="p">,</span><span class="s2">&quot;boot&quot;</span><span class="p">]</span>
<span class="n">X_val</span><span class="p">,</span><span class="n">X_train</span> <span class="o">=</span> <span class="n">X_train_full</span><span class="p">[:</span><span class="mi">5000</span><span class="p">]</span><span class="o">/</span><span class="mf">255.0</span><span class="p">,</span><span class="n">X_train_full</span><span class="p">[</span><span class="mi">5000</span><span class="p">:]</span><span class="o">/</span><span class="mf">255.0</span>
<span class="n">y_val</span><span class="p">,</span><span class="n">y_train</span> <span class="o">=</span> <span class="n">y_train_full</span><span class="p">[:</span><span class="mi">5000</span><span class="p">],</span><span class="n">y_train_full</span><span class="p">[</span><span class="mi">5000</span><span class="p">:]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">history</span> <span class="o">=</span> <span class="n">model_B_on_A</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span><span class="n">epochs</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span><span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_val</span><span class="p">,</span><span class="n">y_val</span><span class="p">))</span>

<span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">model_B_on_A</span><span class="o">.</span><span class="n">layers</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
    <span class="n">layer</span><span class="o">.</span><span class="n">trainable</span><span class="o">=</span><span class="kc">True</span>
    
<span class="n">model_B_on_A</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s2">&quot;sparse_categorical_crossentropy&quot;</span><span class="p">,</span>
             <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
             <span class="n">metrics</span><span class="o">=</span><span class="s2">&quot;accuracy&quot;</span><span class="p">)</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">model_B_on_A</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span><span class="n">epochs</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span><span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_val</span><span class="p">,</span><span class="n">y_val</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/4
1719/1719 [==============================] - 2s 1ms/step - loss: 0.9090 - accuracy: 0.7552 - val_loss: 0.2985 - val_accuracy: 0.8984
Epoch 2/4
1719/1719 [==============================] - 2s 1ms/step - loss: 0.3264 - accuracy: 0.8881 - val_loss: 0.2867 - val_accuracy: 0.9000
Epoch 3/4
1719/1719 [==============================] - 2s 1ms/step - loss: 0.2945 - accuracy: 0.8956 - val_loss: 0.2802 - val_accuracy: 0.9000
Epoch 4/4
1719/1719 [==============================] - 2s 1ms/step - loss: 0.2962 - accuracy: 0.8929 - val_loss: 0.2786 - val_accuracy: 0.9032
Epoch 1/16
1719/1719 [==============================] - 3s 1ms/step - loss: 0.2810 - accuracy: 0.8985 - val_loss: 0.2724 - val_accuracy: 0.9040
Epoch 2/16
1719/1719 [==============================] - 3s 1ms/step - loss: 0.2684 - accuracy: 0.9006 - val_loss: 0.2707 - val_accuracy: 0.9070
Epoch 3/16
1719/1719 [==============================] - 2s 1ms/step - loss: 0.2586 - accuracy: 0.9023 - val_loss: 0.2703 - val_accuracy: 0.9032
Epoch 4/16
1719/1719 [==============================] - 2s 1ms/step - loss: 0.2571 - accuracy: 0.9036 - val_loss: 0.2674 - val_accuracy: 0.9052
Epoch 5/16
1719/1719 [==============================] - 3s 1ms/step - loss: 0.2552 - accuracy: 0.9038 - val_loss: 0.2661 - val_accuracy: 0.9072
Epoch 6/16
1719/1719 [==============================] - 2s 1ms/step - loss: 0.2531 - accuracy: 0.9050 - val_loss: 0.2652 - val_accuracy: 0.9062
Epoch 7/16
1719/1719 [==============================] - 3s 1ms/step - loss: 0.2516 - accuracy: 0.9051 - val_loss: 0.2661 - val_accuracy: 0.9070
Epoch 8/16
1719/1719 [==============================] - 2s 1ms/step - loss: 0.2484 - accuracy: 0.9072 - val_loss: 0.2669 - val_accuracy: 0.9066
Epoch 9/16
1719/1719 [==============================] - 2s 1ms/step - loss: 0.2463 - accuracy: 0.9069 - val_loss: 0.2664 - val_accuracy: 0.9080
Epoch 10/16
1719/1719 [==============================] - 3s 1ms/step - loss: 0.2485 - accuracy: 0.9058 - val_loss: 0.2654 - val_accuracy: 0.9062
Epoch 11/16
1719/1719 [==============================] - 3s 1ms/step - loss: 0.2426 - accuracy: 0.9081 - val_loss: 0.2646 - val_accuracy: 0.9080
Epoch 12/16
1719/1719 [==============================] - 2s 1ms/step - loss: 0.2454 - accuracy: 0.9066 - val_loss: 0.2650 - val_accuracy: 0.9086
Epoch 13/16
1719/1719 [==============================] - 2s 1ms/step - loss: 0.2483 - accuracy: 0.9052 - val_loss: 0.2644 - val_accuracy: 0.9096
Epoch 14/16
1719/1719 [==============================] - 2s 1ms/step - loss: 0.2398 - accuracy: 0.9099 - val_loss: 0.2651 - val_accuracy: 0.9084
Epoch 15/16
1719/1719 [==============================] - 2s 1ms/step - loss: 0.2488 - accuracy: 0.9064 - val_loss: 0.2658 - val_accuracy: 0.9092
Epoch 16/16
1719/1719 [==============================] - 2s 1ms/step - loss: 0.2469 - accuracy: 0.9066 - val_loss: 0.2640 - val_accuracy: 0.9094
</pre></div>
</div>
</div>
</div>
<p>Fully connected DNNs tend to not generalize well. Transfer learning works best with deep CNNs.</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="optimizers">
<h1>Optimizers<a class="headerlink" href="#optimizers" title="Permalink to this headline">#</a></h1>
<p>Training is super slow. You can speed up training with the following strategies:</p>
<ul class="simple">
<li><p>good initialization</p></li>
<li><p>good activation function</p></li>
<li><p>use Batch Normalization</p></li>
<li><p>use Lasso Regression for sparse models.</p></li>
</ul>
<p>Optimizer comparisons:</p>
<ul class="simple">
<li><p><strong>SGD</strong>. Slow convergence speed but high convergence quality. Default in keras is <code class="docutils literal notranslate"><span class="pre">keras.optimizers.SGD()</span></code>. Default is constant learning rate, but one can impose an evolution of the learning rate as training goes. <code class="docutils literal notranslate"><span class="pre">keras.optimizers.SGD(lr=0.01,decay=1e-4)</span></code> implements are power scheduling of the learning rate <span class="math notranslate nohighlight">\(\eta\)</span> ,<span class="math notranslate nohighlight">\(\eta(t)=\eta_0/(1+t/s)^c\)</span>, where <span class="math notranslate nohighlight">\(c\)</span> is the power of the decay (<span class="math notranslate nohighlight">\(1/s\)</span>). There are other functions that can be implemented, check out the Keras information: <a class="reference external" href="https://keras.io/api/callbacks/learning_rate_scheduler/">https://keras.io/api/callbacks/learning_rate_scheduler/</a></p></li>
<li><p><strong>momentum optimization</strong>. The weights are updated by a momentum vector that carries the sum of the gradients, it’s a lot faster though bounces around a bit at the end.  <code class="docutils literal notranslate"><span class="pre">keras.optimizers.SGD(lr=0.001,momentum=0.9)</span></code>. Usually 0.9 works great, but it should be tuned as a hyperparmeter</p></li>
<li><p><strong>Nesterov Accelerated Gradient</strong>. It’s momentum + the gradient of the momentum. NAG is much faster than momentum GD. <code class="docutils literal notranslate"><span class="pre">keras.optimizers.SGD(lr=0.001,momentum=0.9,nesterov=True)</span></code>.</p></li>
<li><p><strong>AdaGrad</strong>. It scales the gradient vector to the steepest dimensions. It has an <em>adaptive learning rate</em> because the learning rate depends on the cost function gradient itself. One issue with neural networks is that the learning rate becomes too small, so do NOT use it for DNN.</p></li>
<li><p><strong>Adam Optimization (best choice)</strong>: adaptive moment estimation. It uses both momentum (beta_1 hyper parameter) and sum of gradients (beta_2) as scaling mechanisms for updating the weights. Good default values are : <code class="docutils literal notranslate"><span class="pre">keras.optimizers.Adam(lr=0.001,beta_1=0.9,beta_2=0.999)</span></code>.</p></li>
</ul>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="avoiding-overfitting-for-training-dnns">
<h1>Avoiding Overfitting for training DNNs<a class="headerlink" href="#avoiding-overfitting-for-training-dnns" title="Permalink to this headline">#</a></h1>
<section id="regularizations">
<h2>Regularizations<a class="headerlink" href="#regularizations" title="Permalink to this headline">#</a></h2>
<p>Use l_1 and l_2 regularizations to constrain the connection weights. Use l_1 to force a sparse model. To implement a regularization on each layer,</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s2">&quot;elu&quot;</span><span class="p">,</span><span class="n">kernel_initializer</span><span class="o">=</span><span class="s2">&quot;he_normal&quot;</span><span class="p">,</span>\
                   <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">regularizers</span><span class="o">.</span><span class="n">l2</span><span class="p">(</span><span class="mf">0.01</span><span class="p">))</span> <span class="c1"># this is your new model: here i reset the output layer</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;tensorflow.python.keras.layers.core.Dense at 0x14c9504c0&gt;
</pre></div>
</div>
</div>
</div>
<p>Because stacking layers means calling the keras functions with many hyperparameters, typing each layer by hand is prone to typos and errors. It is recommended to write functions to wrap these calls. In Python, the function <code class="docutils literal notranslate"><span class="pre">functools.partial()</span></code> can perform just that.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>
<span class="n">RegDense</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">,</span>
                   <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span>
                   <span class="n">kernel_initializer</span><span class="o">=</span><span class="s2">&quot;he_normal&quot;</span><span class="p">,</span>
                   <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">regularizers</span><span class="o">.</span><span class="n">l2</span><span class="p">(</span><span class="mf">0.01</span><span class="p">))</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span> <span class="p">[</span>
<span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">[</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">]),</span> <span class="c1"># reshape the 2D matrix into a 1D vector, without modifying the values</span>
<span class="n">RegDense</span><span class="p">(</span><span class="mi">300</span><span class="p">),</span>
<span class="n">RegDense</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span>
<span class="n">RegDense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s2">&quot;softmax&quot;</span><span class="p">,</span><span class="n">kernel_initializer</span><span class="o">=</span><span class="s2">&quot;glorot_uniform&quot;</span><span class="p">)</span> <span class="p">])</span> <span class="c1"># output layer, 10 neurons since there are 10 classes.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;sequential_11&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_5 (Flatten)          (None, 784)               0         
_________________________________________________________________
dense_22 (Dense)             (None, 300)               235500    
_________________________________________________________________
dense_23 (Dense)             (None, 100)               30100     
_________________________________________________________________
dense_24 (Dense)             (None, 10)                1010      
=================================================================
Total params: 266,610
Trainable params: 266,610
Non-trainable params: 0
_________________________________________________________________
None
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="dropout">
<h1>Dropout<a class="headerlink" href="#dropout" title="Permalink to this headline">#</a></h1>
<p>Most popular regularization technique for DNN. It is muting a portion of the neurons during the training, forcing the gradients to pass through neurons that could be otherwise ignored.</p>
<p><img alt="Multi Layer Perceptron" src="Chapter4-DeepLearning/dropout2.svg" /></p>
<p>Monte Carlo Dropout serves the other purpose ot estimating an ensemble of testing. We make N predections over the test set, setting <code class="docutils literal notranslate"><span class="pre">training=True</span></code> to ensure that the dropout layer is still active and stack the predictions. Because the dropout is active, all the predictions woll be different. Averaging over multiple predictions with dropout gives us a MC estimate that is more reliable than the result of a single prediction.</p>
<section id="marine-will-update-this-section">
<h2>marine will update this section<a class="headerlink" href="#marine-will-update-this-section" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span> <span class="p">[</span>
<span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">[</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">]),</span> <span class="c1"># reshape the 2D matrix into a 1D vector, without modifying the values</span>
<span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="o">=</span><span class="mf">0.2</span><span class="p">),</span>
<span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">300</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">),</span> <span class="c1"># </span>
<span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="o">=</span><span class="mf">0.2</span><span class="p">),</span>
<span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">),</span> <span class="c1"># 100 neurons</span>
<span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="o">=</span><span class="mf">0.2</span><span class="p">),</span>
<span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s2">&quot;softmax&quot;</span><span class="p">)</span> <span class="p">])</span> <span class="c1"># output layer, 10 neurons since there are 10 classes.</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.005</span><span class="c1"># learning rate lr</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span><span class="n">beta_1</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span><span class="n">beta_2</span><span class="o">=</span><span class="mf">0.999</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s2">&quot;sparse_categorical_crossentropy&quot;</span><span class="p">,</span>
             <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
             <span class="n">metrics</span><span class="o">=</span><span class="s2">&quot;accuracy&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># checkpoints_cb = keras.callbacks.ModelCheckpoint(&quot;my_first_NN_model.h5&quot;,save_best_only=True)</span>
<span class="c1"># with the argument save_best_only, the checkpoint saved will be one that of best performance according to the performance metrics chosen.</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span><span class="n">epochs</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span><span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_val</span><span class="p">,</span><span class="n">y_val</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/30
1719/1719 [==============================] - 3s 2ms/step - loss: 0.7496 - accuracy: 0.7258 - val_loss: 0.4039 - val_accuracy: 0.8554
Epoch 2/30
1719/1719 [==============================] - 3s 2ms/step - loss: 0.4648 - accuracy: 0.8286 - val_loss: 0.3678 - val_accuracy: 0.8668
Epoch 3/30
1719/1719 [==============================] - 3s 2ms/step - loss: 0.4165 - accuracy: 0.8466 - val_loss: 0.3525 - val_accuracy: 0.8734
Epoch 4/30
1719/1719 [==============================] - 3s 2ms/step - loss: 0.3932 - accuracy: 0.8533 - val_loss: 0.3384 - val_accuracy: 0.8796
Epoch 5/30
1719/1719 [==============================] - 3s 2ms/step - loss: 0.3777 - accuracy: 0.8596 - val_loss: 0.3319 - val_accuracy: 0.8740
Epoch 6/30
1719/1719 [==============================] - 3s 2ms/step - loss: 0.3693 - accuracy: 0.8622 - val_loss: 0.3219 - val_accuracy: 0.8802
Epoch 7/30
1719/1719 [==============================] - 3s 2ms/step - loss: 0.3561 - accuracy: 0.8662 - val_loss: 0.3194 - val_accuracy: 0.8810
Epoch 8/30
1719/1719 [==============================] - 3s 2ms/step - loss: 0.3479 - accuracy: 0.8685 - val_loss: 0.3089 - val_accuracy: 0.8856
Epoch 9/30
1719/1719 [==============================] - 3s 2ms/step - loss: 0.3382 - accuracy: 0.8733 - val_loss: 0.3045 - val_accuracy: 0.8894
Epoch 10/30
1719/1719 [==============================] - 3s 2ms/step - loss: 0.3310 - accuracy: 0.8743 - val_loss: 0.3066 - val_accuracy: 0.8856
Epoch 11/30
1719/1719 [==============================] - 3s 2ms/step - loss: 0.3249 - accuracy: 0.8812 - val_loss: 0.3020 - val_accuracy: 0.8892
Epoch 12/30
1719/1719 [==============================] - 3s 2ms/step - loss: 0.3227 - accuracy: 0.8804 - val_loss: 0.2938 - val_accuracy: 0.8922
Epoch 13/30
1719/1719 [==============================] - 3s 2ms/step - loss: 0.3196 - accuracy: 0.8793 - val_loss: 0.2980 - val_accuracy: 0.8892
Epoch 14/30
1719/1719 [==============================] - 3s 2ms/step - loss: 0.3094 - accuracy: 0.8823 - val_loss: 0.2992 - val_accuracy: 0.8930
Epoch 15/30
1719/1719 [==============================] - 3s 2ms/step - loss: 0.3101 - accuracy: 0.8843 - val_loss: 0.2926 - val_accuracy: 0.8938
Epoch 16/30
1719/1719 [==============================] - 3s 2ms/step - loss: 0.3168 - accuracy: 0.8825 - val_loss: 0.2919 - val_accuracy: 0.8930
Epoch 17/30
1719/1719 [==============================] - 3s 2ms/step - loss: 0.2979 - accuracy: 0.8867 - val_loss: 0.2839 - val_accuracy: 0.8936
Epoch 18/30
1719/1719 [==============================] - 3s 2ms/step - loss: 0.3069 - accuracy: 0.8849 - val_loss: 0.2867 - val_accuracy: 0.8968
Epoch 19/30
1719/1719 [==============================] - 3s 2ms/step - loss: 0.2986 - accuracy: 0.8872 - val_loss: 0.2906 - val_accuracy: 0.8936
Epoch 20/30
1719/1719 [==============================] - 3s 2ms/step - loss: 0.3006 - accuracy: 0.8897 - val_loss: 0.2941 - val_accuracy: 0.8972
Epoch 21/30
1719/1719 [==============================] - 3s 2ms/step - loss: 0.2940 - accuracy: 0.8904 - val_loss: 0.2906 - val_accuracy: 0.8976
Epoch 22/30
1719/1719 [==============================] - 3s 2ms/step - loss: 0.2865 - accuracy: 0.8926 - val_loss: 0.2886 - val_accuracy: 0.8948
Epoch 23/30
1719/1719 [==============================] - 3s 2ms/step - loss: 0.2908 - accuracy: 0.8888 - val_loss: 0.2904 - val_accuracy: 0.8948
Epoch 24/30
1719/1719 [==============================] - 3s 2ms/step - loss: 0.2843 - accuracy: 0.8919 - val_loss: 0.2968 - val_accuracy: 0.8972
Epoch 25/30
1719/1719 [==============================] - 3s 2ms/step - loss: 0.2841 - accuracy: 0.8927 - val_loss: 0.2901 - val_accuracy: 0.8966
Epoch 26/30
1719/1719 [==============================] - 3s 2ms/step - loss: 0.2747 - accuracy: 0.8970 - val_loss: 0.2877 - val_accuracy: 0.8986
Epoch 27/30
1719/1719 [==============================] - 3s 2ms/step - loss: 0.2833 - accuracy: 0.8922 - val_loss: 0.2958 - val_accuracy: 0.8944
Epoch 28/30
1719/1719 [==============================] - 3s 2ms/step - loss: 0.2801 - accuracy: 0.8931 - val_loss: 0.2931 - val_accuracy: 0.8986
Epoch 29/30
1719/1719 [==============================] - 3s 2ms/step - loss: 0.2826 - accuracy: 0.8926 - val_loss: 0.3005 - val_accuracy: 0.8992
Epoch 30/30
1719/1719 [==============================] - 3s 2ms/step - loss: 0.2754 - accuracy: 0.8963 - val_loss: 0.2809 - val_accuracy: 0.9008
</pre></div>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./Chapter4-DeepLearning"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="ModelTraining.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Training Models</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="neural_networks_PyTorch.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Using PyTorch to build, train, and use neural networks: A short introduction</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By eScience Institute, University of Washington<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>