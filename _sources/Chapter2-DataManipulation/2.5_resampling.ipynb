{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resampling Methods for Basic Regression\n",
    "\n",
    "Data has natural variability that often needs to be accounted for in statistical analysis. There are several strategies to map the data variability into the data-inferred model using resampling methods. \n",
    "Resampling methods involve repeatedly drawing samples from a training set and re-fitting a model. The collection of best-fit models provide a confidence in model fitting.\n",
    "\n",
    "First off all, import some useful modules:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, zipfile, io, gzip, glob, os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Data: Geodetic time series\n",
    "For this tutorial, we use a GPS time series in the Pacific Northwest and explore the long term motions due to the Cascadia subduction zone. We will download GPS time series from the University of Nevada - Reno, [data center](!http://geodesy.unr.edu/NGLStationPages/gpsnetmap/GPSNetMap.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sta=\"P395\"\n",
    "file_url=\"http://geodesy.unr.edu/gps_timeseries/tenv/IGS14/\"+ sta + \".tenv\"\n",
    "r = requests.get(file_url).text.splitlines()  # download, read text, split lines into a list\n",
    "ue=[];un=[];uv=[];se=[];sn=[];sv=[];date=[];date_year=[];df=[]\n",
    "for iday in r:  # this loops through the days of data\n",
    "    crap=iday.split()\n",
    "    if len(crap)<10:\n",
    "      continue\n",
    "    date.append((crap[1]))\n",
    "    date_year.append(float(crap[2]))\n",
    "    ue.append(float(crap[6])*1000)\n",
    "    un.append(float(crap[7])*1000)\n",
    "    uv.append(float(crap[8])*1000)\n",
    "#             # errors\n",
    "    se.append(float(crap[10])*1000)\n",
    "    sn.append(float(crap[11])*1000)\n",
    "    sv.append(float(crap[12])*1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "  # make dataframe\n",
    "crap={'station':sta,'date':date,'date_year':date_year,'east':ue,'north':un,'up':uv}\n",
    "if len(df)==0:\n",
    "    df = pd.DataFrame(crap, columns = ['station', 'date','date_year','east','north','up'])\n",
    "else:\n",
    "    df=pd.concat([df,pd.DataFrame(crap, columns = ['station', 'date','date_year','east','north','up'])])\n",
    "df.describe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the GPS time series\n",
    "fig,ax=plt.subplots(3,1,figsize=(11,8),sharex=True)\n",
    "ax[0].plot(df['date_year'][df['station']==sta],df['east'][df['station']==sta]);ax[0].grid(True);ax[0].set_ylabel('East (mm)')\n",
    "ax[1].plot(df['date_year'][df['station']==sta],df['north'][df['station']==sta]);ax[1].grid(True);ax[1].set_ylabel('North (mm)')\n",
    "ax[2].plot(df['date_year'][df['station']==sta],df['up'][df['station']==sta]);ax[2].grid(True);ax[2].set_ylabel('Up (mm)')\n",
    "ax[2].set_xlabel('Time (years)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Linear Regression\n",
    "\n",
    "There is a clearn **linear trend** in the horizontal position data over the course of the past 16 years. Obviously we could fit the data as such:\n",
    "\n",
    "$E(t) = V_e t + u_e,$\\\n",
    "$N(t) = V_n t + u_n,$\n",
    "\n",
    "where $t$ is time, and we want to regress the data to find the coefficients $V_e$, $u_e$, $V_n$, $u_n$. The displacements are mostly westward, so we will just focus on the East $E(t)$ component for this exercise. The coefficients $u_e$ and $u_n$ are the intercept at $t=0$. They are not zero in this case because $t$ starts in 2006. The coefficients $V_e$ and $V_n$ have the dimension of velocities:\n",
    "\n",
    "$V_e \\sim E(t) / t$ , $V_n \\sim N(t) / t$ ,\n",
    "\n",
    "therefore, we will use this example to discuss a simple **linear regression** and resampling method. We will use both a Scipy function and a Scikit-learn function.\n",
    "\n",
    "To measure a fit performance, we will measure how well the variance is reduced by fitting the data (scatter points) against the model. The variance is:\n",
    "\n",
    "$\\text{Var}(x) = 1/n \\sum_{i=1}^n (x_i-\\hat{x})^2$, \n",
    "\n",
    "where $\\hat{x}$ is the mean of $x$.  When fitting the regression, we predict the values $x_{pred}$. The residuals are the differences between the data and the predicted values: $e = x - x_{pred} $. $R^2$ or <i> coefficient of determination</i> is:\n",
    "\n",
    "$R^2 = 1 - \\text{Var}(x-x_{pred}) /\\text{Var}(x)  = 1 - \\text{Var}(e) /\\text{Var}(x) $\n",
    "The smaller the error, the \"better\" the fit (we will discuss later that a fit can be too good!), the closter $R^2$ is to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's find the trends and detrend the data.\n",
    "from scipy import stats\n",
    "# linear regression such that: displacement = Velocity * time\n",
    "# velocity in the East componentone.\n",
    "Ve, intercept, r_value, p_value, std_err = stats.linregress(df['date_year'][df['station']==sta],df['east'][df['station']==sta])\n",
    "# horizontal plate motion:\n",
    "print(sta,\"overall plate motion there\",Ve,'mm/year')\n",
    "print(\"parameters: Coefficient of determination %f4.2, P-value %f4.2, standard deviation of errors %f4.2\"\\\n",
    "      %(r_value,p_value,std_err))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the ``scikit-learn`` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "# convert the data into numpy arrays.\n",
    "E = np.asarray(df['east'][df['station']==sta]).reshape(-1, 1)# reshaping was necessary to be an argument of Linear regress\n",
    "t = np.asarray(df['date_year'][df['station']==sta]).reshape(-1, 1)\n",
    "tt = np.linspace(np.min(t),np.max(t),1000)\n",
    "\n",
    "# perform the linear regression. First we will use the entire available data\n",
    "regr = LinearRegression()\n",
    "# we will first perform the fit:\n",
    "regr.fit(t,E)\n",
    "# We will first predict the fit:\n",
    "Epred=regr.predict(t) \n",
    "\n",
    "# The coefficients\n",
    "print('Coefficient / Velocity eastward (mm/year): ', regr.coef_[0][0])\n",
    "\n",
    "plt.plot(t,E);ax[0].grid(True);ax[0].set_ylabel('East (mm)')\n",
    "plt.plot(t,Epred,color=\"red\")\n",
    "plt.grid(True)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the errors of the model fit using the module ``sklearn``, we will use the following function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "# The mean squared error\n",
    "print('Mean squared error (mm): %.2f'\n",
    "      % mean_squared_error(E, Epred))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print('Coefficient of determination: %.2f'\n",
    "      % r2_score(E, Epred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bootstrapping\n",
    "Commonly used to provide a measure of accuracy of a parameter estimate or of a given statistical learning method. Random sampling **with replacement** cross-validation iterator. Contrary to cross-validation, bootstrapping allows some samples to occur several times in each splits. However a sample that occurs in the train split will never occur in the test split and vice-versa.\n",
    "For linear regression fit, standard errors are automatically calculated in regression toolbox. But if the model is more complicated, bootstrapping provides one way to calculate these errors.\n",
    "\n",
    "Scikit-learn seems to have deprecated the bootstrap function, but we can find ``resample`` in the ``utils`` module. Make sure you use ``replace=True``.\n",
    "For reproducible results, you can select a fixed ``random_state=int`` to be kept in the workflow. Usually bootstrapping is done over many times (unlike K-fold cross validation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "k=1000\n",
    "vel = np.zeros(k) # initalize a vector to store the regression values\n",
    "mse = np.zeros(k)\n",
    "r2s = np.zeros(k)\n",
    "i=0\n",
    "for iik in range(k):    \n",
    "    ii = resample(np.arange(len(E)),replace=True,n_samples=len(E))# new indices\n",
    "    E_b, t_b = E[ii], t[ii]\n",
    "    # now fit the data on the training set.\n",
    "    regr = LinearRegression()\n",
    "    # Fit on training data:\n",
    "    regr.fit(t_b,E_b)\n",
    "    Epred_val=regr.predict(t) # test on the validation set.\n",
    "\n",
    "    # The coefficients\n",
    "    vel[i]= regr.coef_[0][0]\n",
    "    i+=1\n",
    "\n",
    "# the data shows cleary a trend, so the predictions of the trends are close to each other:\n",
    "print(\"mean of the velocity estimates %f4.2 and the standard deviation %f4.2\"%(np.mean(vel),np.std(vel)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Cross validation\n",
    "\n",
    "A model validation technique to assess how generalizable a statistical analysis is to independent data set is **cross-validation**. There are several tutorials on [cross-validation](!https://scikit-learn.org/stable/modules/cross_validation.html).\n",
    "\n",
    "Cross-validation divides the data set between a *training* set and a *validation* set:\n",
    "\n",
    "<div>\n",
    "<img src=\"ValsetApproach.png\" width=\"500\"/>\n",
    "</div>\n",
    "From scikit-learn: concept for training and validation set.\n",
    "\n",
    "Often the validation set ends up understimationg the prediction errors (model uncertainties) because the validation set is often smaller than the training set. To alleviate that, we can perform cross-validation over many folds of selecting a validation and training set.\n",
    "<div>\n",
    "<img src=\"grid_search_cross_validation.png\" width=\"500\"/>\n",
    "</div>\n",
    "*From scikit-learn*\n",
    "\n",
    "\n",
    "We will now randomly select a training and validation set using the ``sklearn`` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we randomly select values and split the data between training and validation set.\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "# we split once the data between a training and a validating set \n",
    "n=1 # we do this selectio once\n",
    "v_size = 0.3 # 30% of the data will be randomly selected to be the validation set.\n",
    "\n",
    "rs = ShuffleSplit(n_splits=n, test_size=.3, random_state=0)\n",
    "for train_index, val_index in rs.split(E):\n",
    "    E_train, E_val = E[train_index], E[val_index]\n",
    "    t_train, t_val = t[train_index], t[val_index]\n",
    "plt.scatter(t_train,E_train,marker=\"o\");plt.grid(True);plt.ylabel('East (mm)')\n",
    "plt.scatter(t_val,E_val,marker=\"o\",s=6,c=\"red\")\n",
    "plt.xlabel('Time (years)')\n",
    "plt.title('East component')\n",
    "plt.legend(['training set','validation set'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now fit the data and evaluate the error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now fit the data on the training set.\n",
    "regr = LinearRegression()\n",
    "regr_val = LinearRegression()\n",
    "# Fit on training data:\n",
    "regr.fit(t_train,E_train)\n",
    "# We will first predict the fit:\n",
    "Epred=regr.predict(t_train) \n",
    "Epred_val=regr.predict(t_val) \n",
    "\n",
    "# The coefficients\n",
    "print('Training set: Coefficient / Velocity eastward (mm/year): ', regr.coef_[0][0])\n",
    "\n",
    "print('MSE (mean square error) on training set (mm): %.2f'\n",
    "      % mean_squared_error(Epred, E_train))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print('Coefficient of determination on training set: %.2f'\n",
    "      % r2_score(Epred, E_train))\n",
    "\n",
    "print('MSE on validation set (mm): %.2f and coefficient of determiniation on %.2f' %(mean_squared_error(Epred_val, E_val), r2_score(Epred_val, E_val)))\n",
    "\n",
    "\n",
    "plt.plot(t,E);plt.grid(True);plt.ylabel('East (mm)')\n",
    "plt.plot(t_train,Epred,color=\"red\",linewidth=4)\n",
    "plt.plot(t_val,Epred_val,color=\"green\")\n",
    "plt.legend(['data','fit on training','fit on validation'])\n",
    "plt.title('Random selection for data split')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also select the training and validation to be chronological. If the \"state\" of the data changes through time, this may induce a bias in the training. But let's see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E_train, E_val = E[:2*len(E)//3], E[2*len(E)//3+1:]\n",
    "t_train, t_val = t[:2*len(E)//3], t[2*len(E)//3+1:]\n",
    "plt.scatter(t_train,E_train,marker=\"o\");plt.grid(True);plt.ylabel('East (mm)')\n",
    "plt.scatter(t_val,E_val,marker=\"o\",s=6,c=\"red\")\n",
    "plt.xlabel('Time (years)')\n",
    "plt.title('East component')\n",
    "plt.legend(['training set','validation set'])\n",
    "\n",
    "# now fit the data on the training set.\n",
    "regr = LinearRegression()\n",
    "# Fit on training data:\n",
    "regr.fit(t_train,E_train)\n",
    "# We will first predict the fit:\n",
    "Epred=regr.predict(t) \n",
    "Epred_val=regr.predict(t_val) \n",
    "\n",
    "# The coefficients\n",
    "print(' Training set: Coefficient / Velocity eastward (mm/year): ', regr.coef_[0][0])\n",
    "\n",
    "print('Validation set MSE (mm) and Coef of Determination: %.2f,%.2f' \n",
    "%(mean_squared_error(E_val, Epred_val),r2_score(E_val, Epred_val)))\n",
    "\n",
    "\n",
    "plt.plot(t,Epred,color=\"blue\")\n",
    "plt.plot(t_val,Epred_val,color=\"green\")\n",
    "plt.legend(['data','fit on training','fit on validation'])\n",
    "plt.title('Chronological selection for data split')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(vel,50);plt.grid(True);plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Leave One Out Cross Validation\n",
    "\n",
    "LOOCV splits the data in 2 sets (training and validation) <i>n</i> times (<i>n</i> is the number of data points). At each repeat, the training set is **all but one** data, the validation set is one element.\n",
    "\n",
    "<div>\n",
    "<img src=\"LOOCV.png\" width=\"500\"/>\n",
    "</div>\n",
    "Advantages: it has far less bias with respect to the training data. It does not overestimate the test error. Repeated LOOCV will give the exact same results.\n",
    "\n",
    "Disadvantages: it is computationally intensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "vel = np.zeros(len(E)) # initalize a vector to store the regression values\n",
    "mse_train = np.zeros(len(E)-1)\n",
    "mse_val = np.zeros(len(E))\n",
    "r2s = np.zeros(len(E))\n",
    "i=0\n",
    "for train_index, test_index in loo.split(E):    \n",
    "    E_train, E_val = E[train_index], E[val_index]\n",
    "    t_train, t_val = t[train_index], t[val_index]\n",
    "    # now fit the data on the training set.\n",
    "    regr = LinearRegression()\n",
    "    # Fit on training data:\n",
    "    regr.fit(t_train,E_train)\n",
    "    # We will first predict the fit:\n",
    "    Epred_train=regr.predict(t_train) \n",
    "    Epred_val=regr.predict(t_val) \n",
    "\n",
    "    # The coefficients\n",
    "    vel[i]= regr.coef_[0][0]\n",
    "    mse_train[i]= mean_squared_error(E_train, Epred_train)\n",
    "    mse_val[i]= mean_squared_error(E_val, Epred_val)\n",
    "    r2s[i]=r2_score(E_val, Epred_val)\n",
    "    i+=1\n",
    "\n",
    "# the data shows cleary a trend, so the predictions of the trends are close to each other:\n",
    "print(\"mean of the velocity estimates %f4.2 and the standard deviation %f4.2\"%(np.mean(vel),np.std(vel)))\n",
    "# the test error is the average of the mean-square-errors\n",
    "print(\"CV = %f4.2\"%(np.mean(mse)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. K-fold cross validation\n",
    "\n",
    "Designed to reduce the computational cost of LOOCV. Randomly devide over <i>k</i> groups/folds of approximately equal size. It is typical to use <i>5</i> or <i>10</i>.\n",
    "\n",
    "<div>\n",
    "<img src=\"Kfold.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# let's try on 10-folds, 10 adjacent split of the data.\n",
    "k=10\n",
    "kf = KFold(n_splits=k)\n",
    "\n",
    "vel = np.zeros(k) # initalize a vector to store the regression values\n",
    "mse_train = np.zeros(k)\n",
    "mse_val = np.zeros(k)\n",
    "r2s = np.zeros(k)\n",
    "i=0\n",
    "for train_index, val_index in kf.split(E):    \n",
    "    E_train, E_val = E[train_index], E[val_index]\n",
    "    t_train, t_val = t[train_index], t[val_index]\n",
    "    # now fit the data on the training set.\n",
    "    regr = LinearRegression()\n",
    "    # Fit on training data:\n",
    "    regr.fit(t_train,E_train)\n",
    "    # We will first predict the fit:\n",
    "    Epred_train=regr.predict(t_train) \n",
    "    Epred_val=regr.predict(t_val) \n",
    "\n",
    "    # The coefficients\n",
    "    vel[i]= regr.coef_[0][0]\n",
    "    mse_val[i]= mean_squared_error(E_val, Epred_val)\n",
    "    mse_train[i]= mean_squared_error(E_train, Epred_train)\n",
    "    r2s[i]=r2_score(E_train, Epred_train)\n",
    "    i+=1\n",
    "\n",
    "# the data shows cleary a trend, so the predictions of the trends are close to each other:\n",
    "print(\"mean of the velocity estimates %4.2f and the standard deviation %4.2f\"%(np.mean(vel),np.std(vel)))\n",
    "# the test error is the average of the mean-square-errors\n",
    "print(\"mean MSE for training set : %4.2f and the validation set: %4.2f\"%(np.mean(mse_train),np.mean(mse_val)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('madrona')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1c2df93b363d800c8a9b94963221f1be1d8deaf6a76f83b6b9a486ad05d69583"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
