{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.10 Dimensionality Reduction\n",
    "\n",
    "Ideally, one would not need to extract or select feature in the input data. However, reducing the dimensionality as a separate pre-processing steps may be advantageous:\n",
    "\n",
    "1. The complexity of the algorithm depends on the number of input dimensions and size of the data.\n",
    "2. If some features are unecessary, not extracting them saves compute time\n",
    "3. Simpler models are more robust on small datasets\n",
    "4. Fewer features lead to a better understanding of the data.\n",
    "5. Visualization is easier in fewer dimensions.\n",
    "\n",
    "\n",
    "Feature *selection* finds the dimensions that explain the data without loss of information and ends with a smaller dimensionality of the input data. A *forward selection* approach starts with one variable that decreases the error the most and add one by one. A *backward selection* starts with all variables and removes them one by one.\n",
    "\n",
    "Feature *extraction* finds a new set of dimension as a combination of the original dimensions. They can be supervised or unsupervised depending on the output information. Examples are **Principal Component Analysis**, **Independent Component Analysis**, **Linear Discriminant Analysis**\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
