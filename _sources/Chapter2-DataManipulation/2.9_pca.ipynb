{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "059aa832-245c-4923-b168-e2add62b0222",
   "metadata": {},
   "source": [
    "## 2.9 Principal Component Analysis\n",
    "\n",
    "PCA is an unsupervised learning method that finds the mapping from the input to a lower dimemsional space with minimum loss of information. First, we will download again our GPS time series from Oregon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881389fd-3239-4d3a-b073-ae0723a5624e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import useful modules\n",
    "import requests, zipfile, io, gzip, glob, os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import cm\n",
    "%matplotlib inline\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff7e067-1e70-407a-9a65-463d84333559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data and save into a pandas.\n",
    "sta=\"P395\"\n",
    "file_url=\"http://geodesy.unr.edu/gps_timeseries/tenv/IGS14/\"+ sta + \".tenv\"\n",
    "r = requests.get(file_url).text.splitlines()  # download, read text, split lines into a list\n",
    "ue=[];un=[];uv=[];se=[];sn=[];sv=[];date=[];date_year=[];df=[]\n",
    "for iday in r:  # this loops through the days of data\n",
    "    crap=iday.split()\n",
    "    if len(crap)<10:\n",
    "      continue\n",
    "    date.append((crap[1]))\n",
    "    date_year.append(float(crap[2]))\n",
    "    ue.append(float(crap[6])*1000)\n",
    "    un.append(float(crap[7])*1000)\n",
    "    uv.append(float(crap[8])*1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4af647d-efb8-460b-b92f-f164a8ad26ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the three data dimensions\n",
    "E = np.asarray(ue)# East component\n",
    "N = np.asarray(un)# North component\n",
    "U = np.asarray(uv)# Vertical component\n",
    "\n",
    "\n",
    "fig=plt.figure(figsize=(11,8))\n",
    "ax=fig.add_subplot(projection='3d')\n",
    "# viridis = cm.get_cmap('viridis', len(E))\n",
    "# print(viridis)\n",
    "# ax.scatter(E,N,U,cmap=viridis,norm=True);ax.grid(True)\n",
    "ax.scatter(E,N,U);ax.grid(True)\n",
    "ax.set_xlabel('East motions (mm)')\n",
    "ax.set_ylabel('North motions (mm)')\n",
    "ax.set_zlabel('Vertical motions (mm)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bbe9a7-e64a-4d71-bdd9-36503a419179",
   "metadata": {},
   "source": [
    "PCA idenditfies the axis that accounts for the largest amount of variance in the data. Let's  $\\mathbf{Y} = y_1,\\cdots,y_n $  be the data. The steps to PCA are:\n",
    "* center and standardize the data by substracting mean ($\\mu=\\sum_i^n y_i/n$) and scaling with the variance ($\\sigma^2 = 1/n\\sum_1^n ( y_i^2-\\mu^2 )$). The transformed data becomes: $z_i= y_i/\\sigma$ for all $i$).\n",
    "* Calculate the covariance matrix of the data:\n",
    "\n",
    "\n",
    "\n",
    "PCA uses the *(Singular Value Decomposition (SVD)* to decompose the covariance matrix $\\mathbf{C} $:\n",
    "\n",
    "$\\mathbf{X} = \\mathbf{U} \\Sigma \\mathbf{V}^T ,$\n",
    "\n",
    "where $\\mathbf{V}^T$ contains the eigenvectors, or principal components. The *PCs* are normalized, centered around zero.\n",
    "\n",
    "The *1st principal component* eigenvector that has the highest eigenvalue in the direction that has the highest variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ddd943-7cdb-4fed-8ced-552b2bfa5b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize your data in a matrix\n",
    "X = np.vstack([E,N,U]).transpose()\n",
    "print(U.shape)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843d2a1b-1ae2-48f7-92bc-7ef27fec2769",
   "metadata": {},
   "source": [
    "In *PCA*, the input data is centered but not scaled for each feature before applying the SVD.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00fa19e-43ef-4466-821b-40f3180b3768",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca=PCA(n_components=3).fit(X)# retain all 3 components\n",
    "print(pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54c0de2-d376-4c3f-bfb6-7776ad850388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 3 PCs\n",
    "print(pca.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a84699a-cbf1-46a6-91ea-21a265f24512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 3 PCs' explained variance\n",
    "print(pca.explained_variance_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0225cdbc-bee0-4dc8-933f-bb934aa52633",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_vector(v0, v1, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    arrowprops=dict(arrowstyle='->',\n",
    "                    linewidth=2,\n",
    "                    shrinkA=0, shrinkB=0)\n",
    "    ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
    "\n",
    "# plot data\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
    "v= pca.components_[0,:] * 3 * pca.explained_variance_[0]\n",
    "print(pca.mean_[:-1])\n",
    "print(v[0])\n",
    "draw_vector(pca.mean_[:-1], pca.mean_[:-1] + v[0])\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242e1397-c23d-4b84-9480-f375a3d3e8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 3 PCs' explained variance ratio: how much of the variance is explained by each component\n",
    "print(pca.explained_variance_ratio_)\n",
    "fig,ax=plt.subplots(2,1,figsize=(11,8))\n",
    "ax[0].plot(pca.explained_variance_ratio_)\n",
    "ax[0].set_xlabel('Number of dimensions')\n",
    "ax[0].set_ylabel('Explained variance ')\n",
    "ax[0].set_title('Variance explained with each PC')\n",
    "ax[1].plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "ax[1].set_xlabel('Number of dimensions')\n",
    "ax[1].set_ylabel('Explained variance ')\n",
    "ax[1].set_title('Variance explained with cumulated PCs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958de9fc-ef8e-4d87-875a-a4cd9e43d0a9",
   "metadata": {},
   "source": [
    "Now we see that 97% of the variance is explained by the first PC. We can reduce the dimension of our system by only working with a single dimension. Another way to choose the number of dimensions is to select the minimum of PCs that would explain 95% of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c09351-6f23-4215-b5a5-6d1724e3884d",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.argmax(np.cumsum(pca.explained_variance_ratio_)>=0.95) +1\n",
    "print(\"minimum dimension size to explain 95% of the variance \",d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2b0a52-c599-41ca-84d7-96b68823085f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=d).fit(X)\n",
    "X_pca = pca.transform(X)\n",
    "print(\"original shape:   \", X.shape)\n",
    "print(\"transformed shape:\", X_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5024fcd-922c-43a1-a673-84951f8cbe98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the azimuth of the displacement vector\n",
    "X_new = pca.inverse_transform(X_pca)\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
    "plt.scatter(X_new[:, 0], X_new[:, 1], alpha=0.8)\n",
    "plt.axis('equal');\n",
    "\n",
    "# Print out the azimuth and norm \n",
    "import math\n",
    "azimuth=math.degrees(math.atan2(pca.components_[0][0],pca.components_[0][1]))\n",
    "if azimuth <0:azimuth+=360\n",
    "print(\"direction of the plate \",azimuth,\" degrees from North\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d04dd8-de1c-47e9-afc7-43ee6d85622c",
   "metadata": {},
   "source": [
    "*SVD* can be computationally intensive for larger dimensions. It is recommended to use a **randomized PCA** to approximate the first principal components. Scikit-learn automatically switches to randomized PCA in either the following happens: data size > 500, number of vectors is > 500 and the number of PCs selected is less than 80% of either one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647aa4ae-0c00-47ae-8f15-e85c8e4aa327",
   "metadata": {},
   "source": [
    "## Independent Component Analysis\n",
    "\n",
    "ICA is used to estimate sources given noisy measurements. It is frequently used in Geodesy to isolate contributions from earthquakes and hydrology. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e0d7c0-9003-4dff-879a-231802865b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "from sklearn.decomposition import FastICA\n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(0)\n",
    "n_samples = 2000\n",
    "time = np.linspace(0, 8, n_samples)\n",
    "\n",
    "# create 3 source signals\n",
    "s1 = np.sin(2 * time)  # Signal 1 : sinusoidal signal\n",
    "s2 = np.sign(np.sin(3 * time))  # Signal 2 : square signal\n",
    "s3 = signal.sawtooth(2 * np.pi * time)  # Signal 3: saw tooth signal\n",
    "\n",
    "\n",
    "S = np.c_[s1, s2, s3]\n",
    "S += 0.2 * np.random.normal(size=S.shape)  # Add noise\n",
    "S /= S.std(axis=0)  # Standardize data\n",
    "\n",
    "print(S)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59882032-8754-401f-a9f2-3868aa66bab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mix data\n",
    "# create 3 signals at 3 receivers:\n",
    "A = np.array([[1, 1, 1], [0.5, 2, 1.0], [1.5, 1.0, 2.0]])  # Mixing matrix\n",
    "X = np.dot(S, A.T)  # Generate observations\n",
    "\n",
    "\n",
    "# Compute ICA\n",
    "ica = FastICA(n_components=3)\n",
    "S_ = ica.fit_transform(X)  # Reconstruct signals\n",
    "A_ = ica.mixing_  # Get estimated mixing matrix\n",
    "print(A_,A)\n",
    "# For comparison, compute PCA\n",
    "pca = PCA(n_components=3)\n",
    "H = pca.fit_transform(X)  # Reconstruct signals based on orthogonal components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc634c8-f44b-4383-9004-7a4199a8e32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11,8))\n",
    "models = [X, S, S_, H]\n",
    "names = ['Observations (mixed signal)',\n",
    "         'True Sources',\n",
    "         'ICA recovered signals', \n",
    "         'PCA recovered signals']\n",
    "colors = ['red', 'steelblue', 'orange']\n",
    "for ii, (model, name) in enumerate(zip(models, names), 1):\n",
    "    plt.subplot(4, 1, ii)\n",
    "    plt.title(name)\n",
    "    for sig, color in zip(model.T, colors):\n",
    "        plt.plot(sig, color=color)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee662218-0920-4a44-9bcf-64af0bf6675f",
   "metadata": {},
   "source": [
    "## Other Techniques\n",
    "\n",
    "\n",
    "1. Random Projections\n",
    "https://scikit-learn.org/stable/modules/random_projection.html\n",
    "2. Multidimensional Scaling\n",
    "3. Isomap\n",
    "4. t-Distributed stochastic neighbor embedding\n",
    "5. Linear discriminant analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('madrona')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "1c2df93b363d800c8a9b94963221f1be1d8deaf6a76f83b6b9a486ad05d69583"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
