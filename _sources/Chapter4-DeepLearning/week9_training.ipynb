{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Networks and their training\n",
    "\n",
    "\n",
    ":::{important}\n",
    "⚠️ Under Construction !\n",
    ":::\n",
    "\n",
    "\n",
    "A neural network is considered \"deep\" if it has \"many\" layers (more than 5).\n",
    "\n",
    "Shiny new methods like DNN are reported with amazing performance and accuracy. Be aware that only the best performance get put in papers, the majority of the time is spent with failures on model design and training.\n",
    "\n",
    "A major issue with DNN stems from its dependence on the gradient of the cost functions. Because the gradients uses the chain rule with multiplication of errors in each layer, they can vanish or explode if the variance increases with each layer. It happens that the logistic function (sigmoid) having a mean of 0.5, and saturating quickly to either 0 or 1 (gradients of the sigmoid tends to zero quickly).\n",
    "\n",
    "The training of deep neural networks is effectively a conditioning of the model weights to satisfy model accuracy, generalization, and computational efficiency. To strike that balance requires extensive tuning of hyperparameters, a goal to keep models as simple as possible, while aiming the best generalization. The conditioning of model parameters means that their values are not too small (unless intentionally sparse), mostly not too large, and that the variance of these values remains low.\n",
    "\n",
    "\n",
    "This notebook covers important best practice guidelines for training deep neural networks.\n",
    "* Model initialization\n",
    "* Activation functions\n",
    "* Batch normalization\n",
    "* Optimizers\n",
    "* Transfer learning\n",
    "* Dropout (for training and for epistemic uncertainty calculations)\n",
    "\n",
    "## 1. Layer initialization\n",
    "Glorot and He (2010) proposes an initialization that helps lower the variance of the outputs in each layer and of the model. The number of inputs (fan<sub>in</sub> ) and the numbers of neurons (fan<sub>out</sub>). fan<sub>avg</sub> = (fan<sub>in</sub> +fan<sub>out</sub> )/2. By default, Keras uses a Glorot initialization with a uniform distribution. The following table lists the good pairing between initialization and activation functions.\n",
    "\n",
    "| initialization | activation functions      | distribution width ($\\sigma^2$)|\n",
    "| -------------- | --------------------------|------------------------------- |\n",
    "| Glorot         | None,tanh,logistic,softmax| 1/fan<sub>avg</sub>            |\n",
    "| He             | Relu & variants           | 1/fan<sub>in</sub>             |\n",
    "| LeCun          | SELU                      | 1/fan<sub>in</sub>             |\n",
    "\n",
    "One can initialize the layers using these distributions as kernels: \n",
    "\n",
    "> ```python\n",
    "keras.layers.Dense(300,activation=\"relu\",kernel_initializer=\"he_normal\")\n",
    "\n",
    "\n",
    "\n",
    "or choose a custom layer with a He inialization of uniform distribution based out of fan<sub>ave</sub> instead of fan<sub>in</sub>:\n",
    "\n",
    "> ```python \n",
    "init = keras.initializers.VarianceScaling(scale=2.,model='fan_avg',distribution='uniform')\n",
    "\n",
    "> ```python \n",
    "keras.layers.Dense(300,activation=\"relu\",kernel_initializer=my_custom_init)\n",
    "\n",
    "More information here: https://keras.io/api/layers/initializers/\n",
    "\n",
    "\n",
    "## 2. Activation functions\n",
    "\n",
    "\n",
    "**sigmoid/logistic** : most popular at first for smooth activation, but their choice of saturation is not appropriate for DNN where the chain rule applied to the gradients of hte sigmoid tend to vanish.\n",
    "\n",
    "**ReLu** : Practical, but without other regularization mechanisms (batch normalization and dropout), they tend to kill certain neurons in the network.\n",
    "\n",
    "**LeakyReLU** : A variant of the ReLu that has a weak positive slope (hyper parameter ``alpha``) for negative values. It avoids having a strict zero for negative values and avoids the satuation and vanishing of the gradients. Outperforms ReLU. Hard to tune the hyperparameter $\\alpha$. To use Leaky ReLU, you need to add a layer:\n",
    "\n",
    ">```python\n",
    "keras.layers.Dense(10,kernel_initializer=\"he_normal\")\n",
    "keras.layers.LeakyReLU(alpha=0.2)```\n",
    "\n",
    "\n",
    "**Randomized Leaky ReLU - RReLU** : The hyperparameter $\\alpha$ is randomized during training, fixed during testing. RRLeu acts as a regularizer and reduces the risk of overfitting.\n",
    "\n",
    "**Parametric leaky ReLU (PReLu)**: The hyper parameter ``alpha`` is learned during training. Outperforms ReLU for large data sets, runs the risk to overfit the smaller data sets.\n",
    "\n",
    "**Exponential Linear Unit ELU**: Instead of a linear trend in the negative values (as in the LeakyReLU), it is an exponential that smoothly saturates to a negative value. It outperforms all of the variants of ReLUs: training time reduced, never kills the neurons, smooth everywhere if ``alpha=1`` that helps speed up GD. A major drawback is the computational expense, but it is compensated by faster convergence.\n",
    "\n",
    "**Scaled ELU - SELU** (Klambauer et al, 2017): a scaled variant of ELU. It self-normalizes the network because the output of each layer will tend to preserve a zero mean. However, it requires: input features must be standardized (mean 0, std=1), initialization of weights must be LeCun normal (``kernel_initializer=\"lecun_normal\"``), network must be sequential (no skip connection, nor RNN, no wide nets), layers must be dense.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'activation functions')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "x=np.linspace(-2,2,1000)\n",
    "\n",
    "def sigm(x): # define the sigmoid function\n",
    "    return 1/(1+np.exp(-x))\n",
    "def relu(x): # define the Rectified Linear Unit function\n",
    "    return   np.maximum(x,0)\n",
    "def Leakyrelu(a,x): # define the Leaky Rectified Linear Unit function\n",
    "    return  np.maximum(x,a*x)\n",
    "\n",
    "def Leakyrelu(a,x): # define the Leaky Rectified Linear Unit function\n",
    "    return  np.maximum(x,a*x)\n",
    "\n",
    "def elu(a,x): # define the Exponential Linear Unit function\n",
    "    return  np.maximum(x,a*(np.exp(x)-1))\n",
    "                       \n",
    "s = sigm(x)\n",
    "t = np.tanh(x)\n",
    "r = relu(x)\n",
    "lr = Leakyrelu(0.2,x)\n",
    "e = elu(0.2,x)\n",
    "\n",
    "plt.plot(x,s)\n",
    "plt.plot(x,r)\n",
    "plt.plot(x,lr)\n",
    "plt.plot(x,t)\n",
    "plt.plot(x,e)\n",
    "plt.grid(True)\n",
    "plt.legend(['sigmoid','ReLu','Leaky ReLU','TanH','ELU'])\n",
    "plt.xlim([-2,2])\n",
    "plt.xlabel('X')\n",
    "plt.ylim([-1,2])\n",
    "plt.title('activation functions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the preferred choice should be:\n",
    "\n",
    "**SELU (if dense and sequential) > ELU > Leaky ReLU (and variants) > ReLU > tanh > logistic.**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ReLU is the most implemented, and most packages have optimizers that best perform on ReLU, so a safe choice is ReLU. More information here: https://keras.io/api/layers/activation_layers/\n",
    "\n",
    "# 3. Batch Normalization\n",
    "\n",
    "The BN layer standardizes the inputs (shift and normalize the outputs of the layers). It adds parameters and complexity to the network. The benefits are:\n",
    "* it reduces vanishing/exploding gradients (one can now use saturating activation functions),\n",
    "* networks are less sensitive to the weight initialization,\n",
    "* one can use larger learning rates,\n",
    "* they act as a regularizer (reducing the need for regularization and dropout). \n",
    "\n",
    "Tips for its implementation:\n",
    "* Whether the BN layer should be before or after the activation function seems to depend on the task, so one has to experiment with each data set.\n",
    "* To add the BN layer before the activation function, you must remove the activation function from the hidden layers and add them separately \n",
    "* Because BN removes the bias term from the previous layers, you can remove the bias term from the previous layers ``use_bias=False``.\n",
    "* the hyperparameter ``momentum``  that is used to update the averages <1 but close to one 0.999s for large data sets and small mini batches.\n",
    "* BN layers by default acts on one axis, use the argument ``axis=[1,2]`` for 2D input batch (or flatten your 2D input batch to 1D).\n",
    "\n",
    "Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 300)               235200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               30000     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 270,946\n",
      "Trainable params: 268,578\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential( [\n",
    "keras.layers.Flatten(input_shape=[28,28]), # reshape the 2D matrix into a 1D vector, without modifying the values\n",
    "keras.layers.BatchNormalization(),\n",
    "keras.layers.Dense(300,kernel_initializer=\"he_normal\",use_bias=False), # single dense layer, downsampling from input layer to this year from 784 points to 300.\n",
    "keras.layers.BatchNormalization(),\n",
    "keras.layers.Activation(\"relu\"),\n",
    "keras.layers.Dense(100,kernel_initializer=\"he_normal\",use_bias=False), # 100 neurons\n",
    "keras.layers.BatchNormalization(),\n",
    "keras.layers.Activation(\"relu\"),\n",
    "keras.layers.Dense(10,activation=\"softmax\") ]) # output layer, 10 neurons since there are 10 classes.\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Clipping the gradients\n",
    "If Batch Normalization is tricky to use (e.g. in RNNs), one can simply clip the gradients so that they do not explode. You can clip the value of the gradients by a tunable/hyperparameter value ``clipvalue=1.0`` or by the L2 norm of the gradients ``clipnorm=1.0`` (it preserves the orientation of the gradient. Try both and see what works best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.005# learning rate lr\n",
    "optimizer = keras.optimizers.SGD(lr=alpha,clipvalue=1.0) # stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Transfer learning\n",
    "\n",
    "Transfer learning is useful to improve model generalization, continue training networks, modify already trained networks and adapt. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_7 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_A = keras.models.load_model(\"my_first_NN_model.h5\")\n",
    "print(model_A.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_7 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 100)               0         \n",
      "=================================================================\n",
      "Total params: 265,600\n",
      "Trainable params: 265,600\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_7 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_B_on_A = keras.models.Sequential(model_A.layers[:-1]) # use all of the network exept the last year.\n",
    "print(model_B_on_A.summary())\n",
    "model_B_on_A.add(keras.layers.Dense(10,activation=\"softmax\")) # this is your new model: here i reset the output layer\n",
    "print(model_B_on_A.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A_clone = keras.models.clone_model(model_A) # CLONE MODELS TO AVOID OVERWRITING!\n",
    "model_A_clone.set_weights(model_A.get_weights()) # COPY THE WEIGHT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training model_B_on_A will modify  model_A, thus one has to clone model_A to keep an unmodified version.\n",
    "\n",
    "When you retrain but modified and added layers, there is an inbalance of weights or information: the old model is already optimized, the new layers are random. One can freeze the pre-trained layers to force the training on the newly added layers during a few epochs. One can freeze layers by switching the status of the parameters \"trainable\" from \"True\" to \"False\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_7 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 1,010\n",
      "Non-trainable params: 265,600\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable=False\n",
    "print(model_B_on_A.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.005# learning rate lr\n",
    "optimizer = keras.optimizers.SGD(lr=alpha) # stochastic gradient descent\n",
    "model_B_on_A.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "             optimizer=optimizer,\n",
    "             metrics=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train on a few epochs, then unfreeze the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the fashion MNIST data. it's boring, but it works!\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
    "class_names = [\"tshirt\",\"trousers\",\"pullover\",\"dress\",\"coat\",\"sandal\",\"shirt\",\"sneaker\",\"bag\",\"boot\"]\n",
    "X_val,X_train = X_train_full[:5000]/255.0,X_train_full[5000:]/255.0\n",
    "y_val,y_train = y_train_full[:5000],y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.9090 - accuracy: 0.7552 - val_loss: 0.2985 - val_accuracy: 0.8984\n",
      "Epoch 2/4\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3264 - accuracy: 0.8881 - val_loss: 0.2867 - val_accuracy: 0.9000\n",
      "Epoch 3/4\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2945 - accuracy: 0.8956 - val_loss: 0.2802 - val_accuracy: 0.9000\n",
      "Epoch 4/4\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2962 - accuracy: 0.8929 - val_loss: 0.2786 - val_accuracy: 0.9032\n",
      "Epoch 1/16\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 0.2810 - accuracy: 0.8985 - val_loss: 0.2724 - val_accuracy: 0.9040\n",
      "Epoch 2/16\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 0.2684 - accuracy: 0.9006 - val_loss: 0.2707 - val_accuracy: 0.9070\n",
      "Epoch 3/16\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2586 - accuracy: 0.9023 - val_loss: 0.2703 - val_accuracy: 0.9032\n",
      "Epoch 4/16\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2571 - accuracy: 0.9036 - val_loss: 0.2674 - val_accuracy: 0.9052\n",
      "Epoch 5/16\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 0.2552 - accuracy: 0.9038 - val_loss: 0.2661 - val_accuracy: 0.9072\n",
      "Epoch 6/16\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2531 - accuracy: 0.9050 - val_loss: 0.2652 - val_accuracy: 0.9062\n",
      "Epoch 7/16\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 0.2516 - accuracy: 0.9051 - val_loss: 0.2661 - val_accuracy: 0.9070\n",
      "Epoch 8/16\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2484 - accuracy: 0.9072 - val_loss: 0.2669 - val_accuracy: 0.9066\n",
      "Epoch 9/16\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2463 - accuracy: 0.9069 - val_loss: 0.2664 - val_accuracy: 0.9080\n",
      "Epoch 10/16\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 0.2485 - accuracy: 0.9058 - val_loss: 0.2654 - val_accuracy: 0.9062\n",
      "Epoch 11/16\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 0.2426 - accuracy: 0.9081 - val_loss: 0.2646 - val_accuracy: 0.9080\n",
      "Epoch 12/16\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2454 - accuracy: 0.9066 - val_loss: 0.2650 - val_accuracy: 0.9086\n",
      "Epoch 13/16\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2483 - accuracy: 0.9052 - val_loss: 0.2644 - val_accuracy: 0.9096\n",
      "Epoch 14/16\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2398 - accuracy: 0.9099 - val_loss: 0.2651 - val_accuracy: 0.9084\n",
      "Epoch 15/16\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2488 - accuracy: 0.9064 - val_loss: 0.2658 - val_accuracy: 0.9092\n",
      "Epoch 16/16\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2469 - accuracy: 0.9066 - val_loss: 0.2640 - val_accuracy: 0.9094\n"
     ]
    }
   ],
   "source": [
    "history = model_B_on_A.fit(X_train,y_train,epochs=4,validation_data=(X_val,y_val))\n",
    "\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable=True\n",
    "    \n",
    "model_B_on_A.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "             optimizer=optimizer,\n",
    "             metrics=\"accuracy\")\n",
    "history = model_B_on_A.fit(X_train,y_train,epochs=16,validation_data=(X_val,y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fully connected DNNs tend to not generalize well. Transfer learning works best with deep CNNs.\n",
    "\n",
    "# Optimizers\n",
    "\n",
    "Training is super slow. You can speed up training with the following strategies:\n",
    "* good initialization\n",
    "* good activation function\n",
    "* use Batch Normalization\n",
    "* use Lasso Regression for sparse models.\n",
    "\n",
    "Optimizer comparisons:\n",
    "* **SGD**. Slow convergence speed but high convergence quality. Default in keras is ``keras.optimizers.SGD()``. Default is constant learning rate, but one can impose an evolution of the learning rate as training goes. ``keras.optimizers.SGD(lr=0.01,decay=1e-4)`` implements are power scheduling of the learning rate $\\eta$ ,$\\eta(t)=\\eta_0/(1+t/s)^c$, where $c$ is the power of the decay ($1/s$). There are other functions that can be implemented, check out the Keras information: https://keras.io/api/callbacks/learning_rate_scheduler/\n",
    "* **momentum optimization**. The weights are updated by a momentum vector that carries the sum of the gradients, it's a lot faster though bounces around a bit at the end.  ``keras.optimizers.SGD(lr=0.001,momentum=0.9)``. Usually 0.9 works great, but it should be tuned as a hyperparmeter\n",
    "* **Nesterov Accelerated Gradient**. It's momentum + the gradient of the momentum. NAG is much faster than momentum GD. ``keras.optimizers.SGD(lr=0.001,momentum=0.9,nesterov=True)``.\n",
    "* **AdaGrad**. It scales the gradient vector to the steepest dimensions. It has an *adaptive learning rate* because the learning rate depends on the cost function gradient itself. One issue with neural networks is that the learning rate becomes too small, so do NOT use it for DNN.\n",
    "* **Adam Optimization (best choice)**: adaptive moment estimation. It uses both momentum (beta_1 hyper parameter) and sum of gradients (beta_2) as scaling mechanisms for updating the weights. Good default values are : ``keras.optimizers.Adam(lr=0.001,beta_1=0.9,beta_2=0.999)``.\n",
    "\n",
    "# Avoiding Overfitting for training DNNs\n",
    "\n",
    "## Regularizations\n",
    "Use l_1 and l_2 regularizations to constrain the connection weights. Use l_1 to force a sparse model. To implement a regularization on each layer, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x14c9504c0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.Dense(100,activation=\"elu\",kernel_initializer=\"he_normal\",\\\n",
    "                   kernel_regularizer=keras.regularizers.l2(0.01)) # this is your new model: here i reset the output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because stacking layers means calling the keras functions with many hyperparameters, typing each layer by hand is prone to typos and errors. It is recommended to write functions to wrap these calls. In Python, the function ``functools.partial()`` can perform just that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_5 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "RegDense = partial(keras.layers.Dense,\n",
    "                   activation=\"relu\",\n",
    "                   kernel_initializer=\"he_normal\",\n",
    "                   kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "\n",
    "model = keras.models.Sequential( [\n",
    "keras.layers.Flatten(input_shape=[28,28]), # reshape the 2D matrix into a 1D vector, without modifying the values\n",
    "RegDense(300),\n",
    "RegDense(100),\n",
    "RegDense(10,activation=\"softmax\",kernel_initializer=\"glorot_uniform\") ]) # output layer, 10 neurons since there are 10 classes.\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout\n",
    "\n",
    "Most popular regularization technique for DNN. It is muting a portion of the neurons during the training, forcing the gradients to pass through neurons that could be otherwise ignored.\n",
    "\n",
    "![Multi Layer Perceptron](dropout2.svg)\n",
    "\n",
    "Monte Carlo Dropout serves the other purpose ot estimating an ensemble of testing. We make N predections over the test set, setting ``training=True`` to ensure that the dropout layer is still active and stack the predictions. Because the dropout is active, all the predictions woll be different. Averaging over multiple predictions with dropout gives us a MC estimate that is more reliable than the result of a single prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### marine will update this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential( [\n",
    "keras.layers.Flatten(input_shape=[28,28]), # reshape the 2D matrix into a 1D vector, without modifying the values\n",
    "keras.layers.Dropout(rate=0.2),\n",
    "keras.layers.Dense(300,activation=\"relu\"), # \n",
    "keras.layers.Dropout(rate=0.2),\n",
    "keras.layers.Dense(100,activation=\"relu\"), # 100 neurons\n",
    "keras.layers.Dropout(rate=0.2),\n",
    "keras.layers.Dense(10,activation=\"softmax\") ]) # output layer, 10 neurons since there are 10 classes.\n",
    "alpha = 0.005# learning rate lr\n",
    "optimizer = keras.optimizers.Adam(lr=0.001,beta_1=0.9,beta_2=0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "             optimizer=optimizer,\n",
    "             metrics=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.7496 - accuracy: 0.7258 - val_loss: 0.4039 - val_accuracy: 0.8554\n",
      "Epoch 2/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4648 - accuracy: 0.8286 - val_loss: 0.3678 - val_accuracy: 0.8668\n",
      "Epoch 3/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4165 - accuracy: 0.8466 - val_loss: 0.3525 - val_accuracy: 0.8734\n",
      "Epoch 4/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3932 - accuracy: 0.8533 - val_loss: 0.3384 - val_accuracy: 0.8796\n",
      "Epoch 5/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3777 - accuracy: 0.8596 - val_loss: 0.3319 - val_accuracy: 0.8740\n",
      "Epoch 6/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3693 - accuracy: 0.8622 - val_loss: 0.3219 - val_accuracy: 0.8802\n",
      "Epoch 7/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3561 - accuracy: 0.8662 - val_loss: 0.3194 - val_accuracy: 0.8810\n",
      "Epoch 8/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3479 - accuracy: 0.8685 - val_loss: 0.3089 - val_accuracy: 0.8856\n",
      "Epoch 9/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3382 - accuracy: 0.8733 - val_loss: 0.3045 - val_accuracy: 0.8894\n",
      "Epoch 10/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3310 - accuracy: 0.8743 - val_loss: 0.3066 - val_accuracy: 0.8856\n",
      "Epoch 11/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3249 - accuracy: 0.8812 - val_loss: 0.3020 - val_accuracy: 0.8892\n",
      "Epoch 12/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3227 - accuracy: 0.8804 - val_loss: 0.2938 - val_accuracy: 0.8922\n",
      "Epoch 13/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3196 - accuracy: 0.8793 - val_loss: 0.2980 - val_accuracy: 0.8892\n",
      "Epoch 14/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3094 - accuracy: 0.8823 - val_loss: 0.2992 - val_accuracy: 0.8930\n",
      "Epoch 15/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3101 - accuracy: 0.8843 - val_loss: 0.2926 - val_accuracy: 0.8938\n",
      "Epoch 16/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3168 - accuracy: 0.8825 - val_loss: 0.2919 - val_accuracy: 0.8930\n",
      "Epoch 17/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2979 - accuracy: 0.8867 - val_loss: 0.2839 - val_accuracy: 0.8936\n",
      "Epoch 18/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3069 - accuracy: 0.8849 - val_loss: 0.2867 - val_accuracy: 0.8968\n",
      "Epoch 19/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2986 - accuracy: 0.8872 - val_loss: 0.2906 - val_accuracy: 0.8936\n",
      "Epoch 20/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3006 - accuracy: 0.8897 - val_loss: 0.2941 - val_accuracy: 0.8972\n",
      "Epoch 21/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2940 - accuracy: 0.8904 - val_loss: 0.2906 - val_accuracy: 0.8976\n",
      "Epoch 22/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2865 - accuracy: 0.8926 - val_loss: 0.2886 - val_accuracy: 0.8948\n",
      "Epoch 23/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2908 - accuracy: 0.8888 - val_loss: 0.2904 - val_accuracy: 0.8948\n",
      "Epoch 24/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2843 - accuracy: 0.8919 - val_loss: 0.2968 - val_accuracy: 0.8972\n",
      "Epoch 25/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2841 - accuracy: 0.8927 - val_loss: 0.2901 - val_accuracy: 0.8966\n",
      "Epoch 26/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2747 - accuracy: 0.8970 - val_loss: 0.2877 - val_accuracy: 0.8986\n",
      "Epoch 27/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2833 - accuracy: 0.8922 - val_loss: 0.2958 - val_accuracy: 0.8944\n",
      "Epoch 28/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2801 - accuracy: 0.8931 - val_loss: 0.2931 - val_accuracy: 0.8986\n",
      "Epoch 29/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2826 - accuracy: 0.8926 - val_loss: 0.3005 - val_accuracy: 0.8992\n",
      "Epoch 30/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2754 - accuracy: 0.8963 - val_loss: 0.2809 - val_accuracy: 0.9008\n"
     ]
    }
   ],
   "source": [
    "# checkpoints_cb = keras.callbacks.ModelCheckpoint(\"my_first_NN_model.h5\",save_best_only=True)\n",
    "# with the argument save_best_only, the checkpoint saved will be one that of best performance according to the performance metrics chosen.\n",
    "history = model.fit(X_train,y_train,epochs=30,validation_data=(X_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
