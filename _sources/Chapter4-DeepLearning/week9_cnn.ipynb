{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2343cd3f",
   "metadata": {},
   "source": [
    "# 4.2 Convolutional Neural Networks\n",
    "\n",
    "\n",
    ":::{important}\n",
    "⚠️ Under Construction !\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9a3efc-5ec8-4b1a-8669-7344fd6df78b",
   "metadata": {},
   "source": [
    "Note on using tensorflow with jupyter. I ran into issues that my python had the modules, but not my ipython. To fix this, I linked the tensorflow to a python kernel:\n",
    "\n",
    "``  python -m ipykernel install --user --name tensorflow --display-name \"Python 3.8 (TF)\"``\n",
    "\n",
    "Then I re-started the jupyter lab with the kernel ``Python 3.8 (TF)``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a22d14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import layers\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387b3420",
   "metadata": {},
   "source": [
    "# 1. Convolutional Neural Networks\n",
    "\n",
    "\n",
    "Just like MLPs, each neuron in a CNN receives some inputs, performs a dot product and optionally follows it with a non-linearity. CNNs are designed to have images as inputs. When using fully connected layers, images have too many samples and a fully connected layer would have too many trainable parameters. CNN layers are not 1D hidden layers, they are volume of neurons.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"figures/cnn2.jpeg\" alt=\"cnn\" style=\"width: 400px;\"/>\n",
    "<center>Figure: A ConvNet arranges its neurons in three dimensions (width, height, depth), as visualized in one of the layers.</center>\n",
    "\n",
    "\n",
    "The classic sequencing of a CNN (or a block) is: Convolutional Layer, Activation Layer, Pooling Layer, Fully Connected Layer.\n",
    "\n",
    "<!-- <img src=\"figures/Convolution.png\" alt=\"cnn\" style=\"width: 400px;\"/> -->\n",
    "## 1.1 Anatonmy of the Conv layer\n",
    "* The **input** \"feature map\" is the input data for a given layer. It is 3D for a 2D convolution. The three dimensions are: *height*, *width*, *depth/channels*. For instance, width and heights are the 2D images (e.g. 32x32 pixels), the depth can be the different RGB (1 image per color R, G, B). The ordering of the dimension of the input is captured in the ``data_format`` argument with ``'channel_first'`` . When including 3D data (RGB of images, or multi channels of geophysical measurement's spectrograms), write out the input shape: batch_shape + (channels, rows, cols) if ``data_format='channels_first'`` or batch_shape + (rows, cols, channels) if ``data_format='channels_last'``. The input size should be divisible by 2 many times (32, 64, 96, 224, 512).\n",
    " \n",
    "* The **filters** are the **convolution kernels** are the dimensionality of the output space. In general, the number of filters is greater than the input number of channels: the network heights and widths usually decreases throughout the networks, thus increasing the depth or number of filters does not increase the complexity too much. \n",
    "* The **kernel size** is a list of 2 integer that specifies the height and width of the 2D convolution kernel. If both integers are equal, just use a single integer value. The kernel is a filter that has weights to apply on the input values, each kernel map as 1 bias. Let's take an example of a 3x3 input feature map, a 2x2 kernel size, a 2x2 output feature map. The highlighted part of the map in blue are those we focus on:\n",
    "\n",
    "<center> <img src=\"figures/convolution.png\" width=\"600\"></center>\n",
    "<center>Convolution with kernel window (Fig. 6.2.1 from Dive into Deep Learning).</center>\n",
    "\n",
    "In the example above, we perform the following operation: the top-left output value is = 0×0+1×1+3×2+4×3=19. Then we repeat the process until all elements of the output map are field. We also repeat this for each filter. Usually prefer using small but many kernels/filters.\n",
    "\n",
    "\n",
    "* The **stride** is the step that the convolution skips when being applying the filters/kernels. Small strides work better in practice. It is one way to reduce the feature map, but the most popular choice for this is to use ***maxpooling***.\n",
    "\n",
    "\n",
    "* The **padding** is set to either ``SAME`` or ``VALID`` depending on whether the edges of the feature map are extended and filled with zeros (same) to fit the total length of the kernel size and stride, or wether they are ignored (valid). Prefer to use ``SAME`` especially for the hidden conv layers to avoid loosing data and feature knowledge.\n",
    "\n",
    "\n",
    "The convolution can be 1D or 2D depending on the array input:\n",
    "\n",
    "When the input a single dimension array (vector, time series), use a Conv1D layer.\n",
    "https://keras.io/api/layers/convolution_layers/convolution1d/\n",
    "\n",
    "\n",
    "When the input is a 2D image (2D array) or a time series with multiple channels (example of a seismogram with 3 components), use Conv2D layers.\n",
    "https://keras.io/api/layers/convolution_layers/convolution2d/\n",
    "\n",
    "Here are examples of a 32x32 image with three channels (RGB) sent to a 64 output channels/filters with a size of kernels of 6 pixels. The padding is \"SAME\" such that the edges of the feature maps are filled with zeros. We write the function for Keras and Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "408aa658-75cb-4043-ab3b-864956d1c584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(3, 64, kernel_size=(6, 6), stride=(1, 1))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keras&TF version\n",
    "keras.layers.Conv2D(filters=64, kernel_size=6, padding='same', activation='relu', input_shape=(32,32,3))\n",
    "\n",
    "# pytorch version\n",
    "torch.nn.Conv2d(in_channels=3, out_channels=64, kernel_size=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9937a2a-8238-4f30-b126-372a940c9e34",
   "metadata": {},
   "source": [
    "## 1.2 Pooling layers\n",
    "\n",
    "**MaxPooling** layers are downsampling layers. It ouputs the max value of each channel of windwos in a feature map. Downsampling reduces the size of the feature-map, as well as to induce spatial-filter hierarchies by making successive convolution layers look at increasingly large windows (in terms of the fraction of the original input they cover). The pooling size is the factor of reduction in the layer size.\n",
    "\n",
    "\n",
    "<center><img src=\"figures/max_pooling.png\" width=\"600\"></center>\n",
    "<center>Maximum pooling (Fig. 6.5.1 from Dive into Deep Learning).</center>\n",
    "\n",
    "<!-- 114 / 2 = 57 and 80 / 2 = 40 -->\n",
    "\n",
    "**General pooling** are other functions, such as average pooling or even L2-norm pooling. Average pooling was often used historically but has recently fallen out of favor compared to the max pooling operation, which has been shown to work better in practice.\n",
    "\n",
    "\n",
    "Pooling is designed to reduce the complexity and model size in order to deal with overfitting and computational expense. Some argue that striding is sufficient. Some also founds that auto-encoders and generative adversarial networks perform better without pooling.\n",
    "\n",
    "\n",
    "## 1.3 Other notes\n",
    "\n",
    "In the convolutional layer, the neurons are not connected to every part of the input data.\n",
    "The anatomy of a Convolutional layer.\n",
    "\n",
    "A dense layer learns global patterns. A convolution layer learns local patterns. Because of that, CNNs are **translation invariant** as they pick part of the image of time series and generalize the learning elsewhere. CNNs learn **hierarchical patterns**: a first layer learns a local pattern, a second layer combines the local features to create a broader scale feature.\n",
    "\n",
    "\n",
    "\n",
    "# 2 Practice on LeNet-5 networks\n",
    "\n",
    "We will create a CNN LeNet architecture (LeCun et al, 1998) to classify images from the fashion MNIST data. We will write it in Keras/Tensorflow and in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "865611e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 28, 28, 1)\n",
      "(55000,)\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "# print(\"x_train shape:\", x_train.shape, \"y_train shape:\", y_train.shape)\n",
    "class_names = [\"tshirt\",\"trousers\",\"pullover\",\"dress\",\"coat\",\"sandal\",\"shirt\",\"sneaker\",\"bag\",\"boot\"]\n",
    "X_val,X_train = X_train_full[:5000]/255.0,X_train_full[5000:]/255.0\n",
    "y_val,y_train = y_train_full[:5000],y_train_full[5000:]\n",
    "# Here we have to add one dimension to the images in order to match the conv2D requirements in Keras.\n",
    "# And we do it for all variables.\n",
    "X_train=X_train[...,None]\n",
    "X_val=X_val[...,None]\n",
    "X_test=X_test[...,None]\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(np.unique(y_train))\n",
    "print(len(np.unique(y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d280968e",
   "metadata": {},
   "source": [
    "### 2.1 Create the model\n",
    "\n",
    "We will create a CNN LeNet-5 architecture (LeCun et al, 1998) that is a sequential stack of 3 convolutional layers, 2 fully connected layers. There are several graphical representations of networks that we often find in the literature, with a few examples below.\n",
    "<center><img src=\"figures/lenet.svg\" width=\"600\"></center>\n",
    "<center>LeNet-5 architecture</center>\n",
    "<center><img src=\"figures/lenet-vert.svg\" width=\"200\"></center>\n",
    "<center>LeNet-5 architecture</center>\n",
    "\n",
    "Using words, we can see that the CNN is composed of an input map of size 28x28 pixels, and the images are in gray scales so there is a single channel. It is followed by a convolution layer of size 28x28 and depth 6 (# of channels) and kernel sizes of 5x5, a pooling layer of size 2 - stride 2, a conv layer of depth 6 (or 6 channels) and kernel size 5x5, another pooling layer of size 2 - stride 2, and then 3 fully connected (dense) layers of respective sizes 120, 84, 10. The activation functions in the original LeNet-5 were sigmoids and the last activation function was a Gaussian function,  which we replaced with softmax. One can test the role of activation functions by changing them to ReLu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "181dcef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 6)         156       \n",
      "_________________________________________________________________\n",
      "average_pooling2d (AveragePo (None, 14, 14, 6)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 14, 14, 16)        2416      \n",
      "_________________________________________________________________\n",
      "average_pooling2d_1 (Average (None, 7, 7, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 120)               94200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                850       \n",
      "=================================================================\n",
      "Total params: 107,786\n",
      "Trainable params: 107,786\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "# Must define the input shape in the first layer of the neural network\n",
    "keras.layers.Conv2D(filters=6, kernel_size=5, padding='same', activation='sigmoid', input_shape=(28,28,1)),\n",
    "keras.layers.AveragePooling2D(2), # you could replace with MaxPooling2D\n",
    "keras.layers.Conv2D(filters=16, kernel_size=5, padding='same', activation='sigmoid'),\n",
    "keras.layers.AveragePooling2D(2),# you could replace with MaxPooling2D\n",
    "keras.layers.Flatten(),\n",
    "keras.layers.Dense(120, activation='sigmoid'),\n",
    "# keras.layers.Dropout(0.5),\n",
    "keras.layers.Dense(84, activation='sigmoid'),\n",
    "# keras.layers.Dropout(0.5),\n",
    "keras.layers.Dense(10, activation='softmax')])\n",
    "# Take a look at the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f17ea87-9855-4d30-9093-7f3dab694347",
   "metadata": {},
   "source": [
    "Model in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92c493aa-e8b3-4449-a45f-07f39f74ff2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reshape(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(-1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1410b406-0ee0-4748-8937-e83d9892d644",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_torch = torch.nn.Sequential(Reshape(), nn.Conv2d(1, 6, kernel_size=5,\n",
    "                                               padding=2), nn.Sigmoid(),\n",
    "                          nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "                          nn.Conv2d(6, 16, kernel_size=5), nn.Sigmoid(),\n",
    "                          nn.AvgPool2d(kernel_size=2, stride=2), nn.Flatten(),\n",
    "                          nn.Linear(16 * 5 * 5, 120), nn.Sigmoid(),\n",
    "                          nn.Linear(120, 84), nn.Sigmoid(), nn.Linear(84, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb26b786-5d3a-45ca-a276-94365a3e9099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial input shape: \t torch.Size([1, 1, 28, 28])\n",
      "Reshape output shape: \t torch.Size([1, 1, 28, 28])\n",
      "Conv2d output shape: \t torch.Size([1, 6, 28, 28])\n",
      "Sigmoid output shape: \t torch.Size([1, 6, 28, 28])\n",
      "AvgPool2d output shape: \t torch.Size([1, 6, 14, 14])\n",
      "Conv2d output shape: \t torch.Size([1, 16, 10, 10])\n",
      "Sigmoid output shape: \t torch.Size([1, 16, 10, 10])\n",
      "AvgPool2d output shape: \t torch.Size([1, 16, 5, 5])\n",
      "Flatten output shape: \t torch.Size([1, 400])\n",
      "Linear output shape: \t torch.Size([1, 120])\n",
      "Sigmoid output shape: \t torch.Size([1, 120])\n",
      "Linear output shape: \t torch.Size([1, 84])\n",
      "Sigmoid output shape: \t torch.Size([1, 84])\n",
      "Linear output shape: \t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(size=(1, 1, 28, 28), dtype=torch.float32)\n",
    "print('Initial input shape: \\t', X.shape)\n",
    "for layer in model_torch:\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__, 'output shape: \\t', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532a14e0",
   "metadata": {},
   "source": [
    "### 2.2 Compile the model\n",
    "choose the appropriate loss function. We have a multi-class classification example, so we will use the categorical crossentropy. We will use the Adam optimizer to be fast. We will use the accuracy for the metric on the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f44abd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cb = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", # use the validation loss as a metric\n",
    "    patience=2, # number of epochs to wait until there is improvement\n",
    "    verbose=1,\n",
    "    mode=\"auto\",\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "             optimizer='adam',\n",
    "             metrics=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0aaf457",
   "metadata": {},
   "source": [
    "### 2.3 Train the model\n",
    "Choose the batch size, the number of epochs (iterations). Fit and be patient: grab a coffee, watch a webinar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9df9b7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2292/2292 [==============================] - 15s 6ms/step - loss: 0.8668 - accuracy: 0.6758 - val_loss: 0.5406 - val_accuracy: 0.8046\n",
      "Epoch 2/20\n",
      "2292/2292 [==============================] - 16s 7ms/step - loss: 0.5054 - accuracy: 0.8112 - val_loss: 0.4286 - val_accuracy: 0.8404\n",
      "Epoch 3/20\n",
      "2292/2292 [==============================] - 18s 8ms/step - loss: 0.4274 - accuracy: 0.8407 - val_loss: 0.3861 - val_accuracy: 0.8540\n",
      "Epoch 4/20\n",
      "2292/2292 [==============================] - 21s 9ms/step - loss: 0.3814 - accuracy: 0.8583 - val_loss: 0.3478 - val_accuracy: 0.8760\n",
      "Epoch 5/20\n",
      "2292/2292 [==============================] - 23s 10ms/step - loss: 0.3533 - accuracy: 0.8691 - val_loss: 0.3277 - val_accuracy: 0.8778\n",
      "Epoch 6/20\n",
      "2292/2292 [==============================] - 25s 11ms/step - loss: 0.3332 - accuracy: 0.8767 - val_loss: 0.3204 - val_accuracy: 0.8830\n",
      "Epoch 7/20\n",
      "2292/2292 [==============================] - 23s 10ms/step - loss: 0.3172 - accuracy: 0.8824 - val_loss: 0.3144 - val_accuracy: 0.8844\n",
      "Epoch 8/20\n",
      "2292/2292 [==============================] - 24s 10ms/step - loss: 0.3038 - accuracy: 0.8868 - val_loss: 0.2963 - val_accuracy: 0.8906\n",
      "Epoch 9/20\n",
      "2292/2292 [==============================] - 26s 11ms/step - loss: 0.2944 - accuracy: 0.8906 - val_loss: 0.2941 - val_accuracy: 0.8898\n",
      "Epoch 10/20\n",
      "2292/2292 [==============================] - 28s 12ms/step - loss: 0.2843 - accuracy: 0.8941 - val_loss: 0.2921 - val_accuracy: 0.8896\n",
      "Epoch 11/20\n",
      "2292/2292 [==============================] - 28s 12ms/step - loss: 0.2745 - accuracy: 0.8978 - val_loss: 0.2804 - val_accuracy: 0.8972\n",
      "Epoch 12/20\n",
      "2292/2292 [==============================] - 30s 13ms/step - loss: 0.2664 - accuracy: 0.9015 - val_loss: 0.2886 - val_accuracy: 0.8898\n",
      "Epoch 13/20\n",
      "2292/2292 [==============================] - 31s 14ms/step - loss: 0.2592 - accuracy: 0.9033 - val_loss: 0.2870 - val_accuracy: 0.8918\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00013: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,y_train,batch_size=24,epochs=20,validation_data=(X_val,y_val),callbacks=cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2d2421",
   "metadata": {},
   "source": [
    "### 2.4 Evaluate the model\n",
    "Plot the acuracy scores as a function of epochs to see how well we train. Note that we used the ``callbacks`` option to stop after a few iterations if there was no more improvememnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcc6f539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 4s 14ms/step - loss: 0.3807 - accuracy: 0.8615\n",
      "\n",
      " Test accuracy: 0.8615000247955322\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "plt.grid(True)\n",
    "plt.xlabel('epochs')\n",
    "\n",
    "# Evaluate the model on test set\n",
    "score = model.evaluate(X_test, y_test, verbose=1)\n",
    "# Print test accuracy\n",
    "print('\\n', 'Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839cc5da-52b1-4246-b87a-a636ebe1efb1",
   "metadata": {},
   "source": [
    "Here is the PyTorch version.\n",
    "First we need to convert from numpy arrays to pytorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bce3413c-22c6-4fef-99f1-2058c75697b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep data for training and testing\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchvision import transforms\n",
    "pXtrain = torch.from_numpy(X_train).float()\n",
    "pXtest = torch.from_numpy(X_test).float()\n",
    "pytrain = torch.from_numpy(y_train).type(torch.LongTensor)\n",
    "pytest = torch.from_numpy(y_test).type(torch.LongTensor)\n",
    "\n",
    "\n",
    "\n",
    "# transform = transforms.Compose([transforms.Resize(64), transforms.ToTensor()])\n",
    "# dataset = MyDataset(data, targets, transform=transform)\n",
    "# dataloader = DataLoader(dataset, batch_size=5)\n",
    "\n",
    "train_data = TensorDataset(pXtrain,pytrain)\n",
    "test_data= TensorDataset(pXtest,pytest)\n",
    "\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_data,batch_size=24)\n",
    "test_dataloader = DataLoader(test_data,batch_size=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ed6ce1e3-943e-49c3-9206-37aafacbde28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function defined to train the model\n",
    "def train(model, n_epochs, lr, trainloader, testloader=None):\n",
    "\n",
    "    # Save loss and accuracy for plotting\n",
    "    loss_time = np.zeros(n_epochs)\n",
    "    accuracy_time = np.zeros(n_epochs)\n",
    "\n",
    "    # Initialize weights\n",
    "    def init_weights(m):\n",
    "        if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "    model.apply(init_weights)\n",
    "\n",
    "    # Define loss and optimization method\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Loop on number of epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        # Initialize the loss\n",
    "        running_loss = 0\n",
    "        model.train()\n",
    "        # Loop on samples in train set\n",
    "        for i, (X, y) in enumerate(trainloader):\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = model(X)\n",
    "            l = loss(y_hat, y)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            # Add the value of the loss for this sample\n",
    "            running_loss += l.item()\n",
    "        # Save loss at the end of each epoch\n",
    "        loss_time[epoch] = running_loss / len(trainloader)\n",
    "\n",
    "        # After each epoch, evaluate the performance on the test set\n",
    "        if testloader is not None:\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # We evaluate the model, so we do not need the gradient\n",
    "            with torch.no_grad():\n",
    "                # Loop on samples in test set\n",
    "                for i, (X, y) in enumerate(testloader):\n",
    "                    y_hat = model(X)\n",
    "                    # Compare predicted label and true label\n",
    "                    _, predicted = torch.max(y_hat.data, 1)\n",
    "                    total += y.size(0)\n",
    "                    correct += (predicted == y).sum().item()\n",
    "            # Save accuracy at the end of each epochs\n",
    "            accuracy_time[epoch] = 100 * correct / total\n",
    "    \n",
    "        # Print intermediate results on screen\n",
    "        if testloader is not None:\n",
    "            print('[Epoch %d] loss: %.3f - accuracy: %.3f' %\n",
    "              (epoch + 1, running_loss / len(trainloader), 100 * correct / total))\n",
    "        else:\n",
    "            print('[Epoch %d] loss: %.3f' %\n",
    "              (epoch + 1, running_loss / len(trainloader)))\n",
    "\n",
    "    # Save history of loss and test accuracy\n",
    "    if testloader is not None:\n",
    "        return (loss_time, accuracy_time)\n",
    "    else:\n",
    "        return (loss_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fce1c03d-9413-43ac-8825-1128d8730273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] loss: 2.314 - accuracy: 10.000\n",
      "[Epoch 2] loss: 1.036 - accuracy: 73.080\n",
      "[Epoch 3] loss: 0.484 - accuracy: 71.310\n",
      "[Epoch 4] loss: 0.402 - accuracy: 72.180\n",
      "[Epoch 5] loss: 0.362 - accuracy: 72.840\n",
      "[Epoch 6] loss: 0.336 - accuracy: 74.160\n",
      "[Epoch 7] loss: 0.317 - accuracy: 76.230\n",
      "[Epoch 8] loss: 0.301 - accuracy: 76.210\n",
      "[Epoch 9] loss: 0.287 - accuracy: 76.670\n",
      "[Epoch 10] loss: 0.276 - accuracy: 77.570\n"
     ]
    }
   ],
   "source": [
    "(loss, accuracy) = train(model_torch, 10, 0.9, train_dataloader, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f48763-9645-4b18-a18c-03dbe916d3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss', color=color)\n",
    "ax1.plot(np.arange(1, 11), loss, color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Correct predictions', color=color)\n",
    "ax2.plot(np.arange(1, 11), accuracy, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf926f36",
   "metadata": {},
   "source": [
    "# 3. Example on seismic data\n",
    "\n",
    "In this class, we will use a simplified version of ConvNetQuake (Perol et al, 2018). The network was designed as a classification that predicted the nature and location of the seismic events. The earthquakes from a known earthquake catalog were clustered using k-means, and each earthquake waveforms were  We will use the two seismic station seismograms already labeled as \"earthquakes\" or \"noise\" to perform.\n",
    "\n",
    "\n",
    "<img src=\"figures/ConvNetQuake.jpg\" alt=\"ConvNetQuake\" style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "### 3.1 read the data\n",
    "data is stored in Gdrive here https://drive.google.com/drive/folders/1LF2_bBHvUyinJuaiMwhG4C_rMErerG80?usp=sharing. Download the data and place it in a \"data\" folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0f48ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load OK029 template data:\n",
    "with h5py.File(\"./data/templates_029.h5\", \"r\") as f:\n",
    "    eq1 = np.asarray(f['earthquakes']);neq1=eq1.shape[0]\n",
    "    no1 = np.asarray(f[\"noise\"])\n",
    "\n",
    "\n",
    "# # load OK027 template data:\n",
    "# with h5py.File(\"./data/templates_027.h5\", \"r\") as f:\n",
    "#     eq2 = np.asarray(f['earthquakes'])\n",
    "#     no2 = np.asarray(f[\"noise\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5205181",
   "metadata": {},
   "source": [
    "### 3.2 Prep the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38db75f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  allocate memory\n",
    "quakes=np.zeros(shape=(eq1.shape[0],1000,3),dtype=np.float32)\n",
    "noise=np.zeros(shape=(no1.shape[0],1000,3),dtype=np.float32)\n",
    "# quakes2=np.zeros(shape=(eq2.shape[0],1000,3),dtype=np.float32)\n",
    "# noise2=np.zeros(shape=(no2.shape[0],1000,3),dtype=np.float32)\n",
    "\n",
    "# Normalize the seismograms to their peak amplitudes\n",
    "for iq in range(eq1.shape[0]):\n",
    "    for ic in range(3):\n",
    "        if np.max(np.abs(eq1[iq,ic,:]))>0:\n",
    "            quakes[iq,:,ic]=eq1[iq,ic,:]/np.max(np.abs(eq1[iq,ic,:]))\n",
    "            \n",
    "for iq in range(no1.shape[0]):\n",
    "    for ic in range(3):\n",
    "        if np.max(np.abs(no1[iq,ic,:]))>0:\n",
    "            noise[iq,:,ic]=no1[iq,ic,:]/np.max(np.abs(no1[iq,ic,:]))\n",
    "\n",
    "# for iq in range(eq2.shape[0]):\n",
    "#     for ic in range(3):\n",
    "#         if np.max(np.abs(eq2[iq,ic,:]))>0:\n",
    "#             quakes2[iq,:,ic]=eq2[iq,ic,:]/np.max(np.abs(eq2[iq,ic,:]))\n",
    "            \n",
    "# for iq in range(no12.shape[0]):\n",
    "#     for ic in range(3):\n",
    "#         if np.max(np.abs(no2[iq,ic,:]))>0:\n",
    "#             noise2[iq,:,ic]=no2[iq,ic,:]/np.max(np.abs(no2[iq,ic,:]))\n",
    "\n",
    "# select data that is strictly positive and finite\n",
    "iq1=np.where( ( np.abs(quakes[:,0,0])>0)&(np.isfinite(quakes[:,0,0])))[0]\n",
    "# iq2=np.where( (np.abs(quakes2[:,0,0])>0)&(np.isfinite(quakes2[:,0,0])))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b034e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label & data\n",
    "y = np.concatenate((np.ones(len(iq1)+len(iq2),dtype=np.int),np.zeros(len(iq1)+len(iq2),dtype=np.int))) # 0 for noise, 1 for event\n",
    "# X = np.zeros(shape=(len(train_labels),1000,3,1))\n",
    "X= np.concatenate((quakes[iq1,:,:],quakes2[iq2,:,:],noise[iq1,:,:],noise2[iq2,:,:]),axis=0)\n",
    "X=X[...,None]# add that depth/channel dimension\n",
    "\n",
    "nlabels=2 # = len(np.unique(y))\n",
    "\n",
    "# Split train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73d6a00",
   "metadata": {},
   "source": [
    "### 3.3 Define ML model\n",
    "ConvNetQuake is a simple stack of 8 conv2d layers with 32 channels, stride of 2 kernel size of 3x3, ReLu activation functions, padding is the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381b875d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the architecture\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(1000,3,1), use_bias=True, strides=2, padding='SAME'))\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', use_bias=True, strides=2, padding='SAME'))\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', use_bias=True, strides=2, padding='SAME'))\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', use_bias=True, strides=2, padding='SAME'))\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', use_bias=True, strides=2, padding='SAME'))\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', use_bias=True, strides=2, padding='SAME'))\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', use_bias=True, strides=2, padding='SAME'))\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', use_bias=True, strides=2, padding='SAME'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(nlabels, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51b7a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the network\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac888ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "history=model.fit(X_train,y_train,validation_split=0.2, epochs=20, batch_size=128,shuffle=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6326d2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for overfitting\n",
    "history_dic = history.history\n",
    "loss_values = history_dic['loss']\n",
    "val_loss_values = history_dic['val_loss']\n",
    "acc_values = history_dic['acc']\n",
    "val_acc_values = history_dic['val_acc']\n",
    "epochs=range(1,len(loss_values)+1)\n",
    "fig,ax=plt.subplots(2)\n",
    "ax[0].plot(epochs,loss_values,'bo',label='Training loss')\n",
    "ax[0].plot(epochs,val_loss_values,'b',label='Validation loss')\n",
    "ax[0].set_title('Training and validation loss')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].legend()\n",
    "ax[1].plot(epochs,acc_values,'bo',label='Training accuracy')\n",
    "ax[1].plot(epochs,val_acc_values,'b',label='Validation accuracy')\n",
    "ax[1].set_title('Training and validation accuracy')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('accuracy')\n",
    "ax[1].legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2ebbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "test=model.evaluate(X_test,y_test,batch_size=64,verbose=1)\n",
    "print('loss and accuracy at test')\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd53a30",
   "metadata": {},
   "source": [
    "## Tuning CNN networks\n",
    "\n",
    "There are many hyperparameters and model choices to make:\n",
    "* learning rate, optimizer, batch_size, activation functions, loss functions\n",
    "* architecture: number of layers, depth of kernels, activation functions, regularization, batch normalization\n",
    "\n",
    "One can treat the hyperparameter search as an optimization problem. Keras tuner (https://keras-team.github.io/keras-tuner/) can be used to randomize the grid search.\n",
    "\n",
    "http://caffe.berkeleyvision.org/model_zoo.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b273ad1-a91e-4dd8-a508-f45f024fe374",
   "metadata": {},
   "source": [
    "# 4. How to read and recode published networks\n",
    "\n",
    "Let us say that you read a research paper explaining the architecture of the convolutional neural network used by the authors to carry out their data analysis. How will you try to reproduce their results? They do not provide a github!\n",
    "\n",
    "Let us look at the following paper:\n",
    "\n",
    "Rouet-Leduc, B., Hulbert, C., McBrearty, I. W., Johnson, P. A. (2020). Probing slow earthquakes with deep learning. Geophysical Research Letters, 47, e2019GL085870. https://doi.org/10.1029/2019GL085870.\n",
    "\n",
    "<img src=\"figures/cnn_rouet-leduc.png\" width=\"600\">\n",
    "<center>Schematic of the CNN and its architecture (Figure 1 from Rouet-Leduc et al. (2020).</center>\n",
    "\n",
    "* **Batch Normalization** => unclear from the paper, but this seems to be the normalization of the data\n",
    "* **Dropout** => unclear from the paper what this is\n",
    "* **Input** Spectrogram = Image with 129 x 95 x1 pixels\n",
    "* **Conv2D** convolution is has a kernel size of 16x16 feature map of size 114x80 is depth 32 (# of channels), activation is ReLU (found in the supplementary material)\n",
    "* **Maxpooling** of size 2\n",
    "* **Dropout** 5%, found in the supplementary material\n",
    "* **Conv2D** of kernel size 8 x 8, depth 64\n",
    "* **Maxpooling** of size 2\n",
    "* **Dropout** 5%, found in the supplementary material\n",
    "* **Full Connected - Dense layers** with 36608 neurons (found in the supplementary material)\n",
    "* **Full Connected - Dense layers** with 10 neurons (found in the supplementary material)\n",
    "* **Full Connected - Dense layers** with 1 neuron, sigmoid activation function (found in the supplementary material)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c4d5a0-1ccf-48b4-b883-14c97fe481c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(nn.Conv2d(in_channels=1, out_channels=32, kernel_size=16),\n",
    "                            nn.ReLU(),\n",
    "                            nn.MaxPool2d(kernel_size=2),\n",
    "                            nn.Dropout(0.05),\n",
    "                            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=8),\n",
    "                            nn.ReLU(),\n",
    "                            nn.MaxPool2d(kernel_size=2),\n",
    "                            nn.Dropout(0.05),\n",
    "                            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=4),\n",
    "                            nn.Flatten(),\n",
    "                            nn.Linear(36608, 10),\n",
    "                            nn.Sigmoid(),\n",
    "                            nn.Linear(10, 1),\n",
    "                            nn.Sigmoid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de06edc7-41b3-431c-8a2f-57973653aaeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "af2b86a0d97d2bdb49befe19981ba48b79a904c391b62d75845b127da778abba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
